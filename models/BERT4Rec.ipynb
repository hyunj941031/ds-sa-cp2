{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunj941031/ds-sa-cp2/blob/main/models/BERT4Rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vg0sx3_eDd5",
        "outputId": "7af72538-01b0-4d01-f29e-51e7ace90f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IatRo2Phd6_n",
        "outputId": "b40ea1ba-425a-475b-d2dd-974734d1ccfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-box\n",
            "  Downloading python_box-7.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-7.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zaXMbpoEeB03"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "torch.set_printoptions(sci_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls46hifUgmt8"
      },
      "source": [
        "# 1. 학습 parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kQr30WC2eQ8E"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : '/content/drive/MyDrive/fashion_campus_dataset',\n",
        "    \n",
        "    'sequence_len' : 50,\n",
        "    'mask_prob' : 0.3, # cloze task\n",
        "    'random_seed' : 123,\n",
        "\n",
        "    'num_layers' : 2, # block 수 (encoder layer 수)\n",
        "    'hidden_units' : 50, # Embedding size\n",
        "    'num_heads' : 2, # Multi-head layer 수 (병렬처리), hidden_units를 나눴을 때 나누어 떨어지게게\n",
        "    'dropout' : 0.15, # dropout의 비율\n",
        "\n",
        "    'epoch' : 5,\n",
        "    'patience' : 5,\n",
        "    'batch_size' : 256,\n",
        "    'lr' : 0.001,\n",
        "\n",
        "    'num_epochs' : 10,\n",
        "    'num_workers' : 4,\n",
        "    'val_data' : 3,\n",
        "    'delete_data' : False,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsuqMA2_gqXH"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pwHqzLnkgf4I"
      },
      "outputs": [],
      "source": [
        "class MakeSequenceDataSet():\n",
        "    \"\"\"\n",
        "    SequenceData 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'user_item.csv'))\n",
        "        if config['delete_data']:\n",
        "            self.df = self.delete_ones()\n",
        "\n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('itemId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_items, self.num_users = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['itemId'].apply(lambda x : self.item_encoder[x] + 1)\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "        self.df = self.df.sort_values(['user_idx', 'timestamp']) # 시간에 따른 정렬\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col:str) -> dict:\n",
        "        '''\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Return:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        '''\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "        \n",
        "        return encoder, decoder\n",
        "\n",
        "    def delete_ones(self) -> dict:\n",
        "        a = self.df.groupby('userId')['itemId'].size()\n",
        "        for i in a.index:\n",
        "            if a[i] <= config['val_data']:\n",
        "                del(a[i])\n",
        "        df_ = self.df.copy()\n",
        "        df_ = df_[df_['userId'].isin(a.index)]\n",
        "                \n",
        "        return df_\n",
        "\n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        '''\n",
        "        sequence data 생성\n",
        "\n",
        "        Return:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        '''\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        group_df = self.df.groupby('user_idx')\n",
        "        for user, item in group_df:\n",
        "            users[user].extend(item['item_idx'].tolist())\n",
        "\n",
        "        for user in users:\n",
        "            user_train[user] = users[user][:-config['val_data']]\n",
        "            user_valid[user] = users[user][-config['val_data']:] # 마지막 아이템 예측\n",
        "\n",
        "        return user_train, user_valid\n",
        "\n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iZf_2Zq9k4ix"
      },
      "outputs": [],
      "source": [
        "class BERTTrainDataSet(Dataset):\n",
        "    def __init__(self, user_train, num_users, num_items, sequence_len=200, mask_prob=0.15, random_seed=None):\n",
        "        self.user_train = user_train\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.sequence_len = sequence_len\n",
        "        self.mask_prob = mask_prob\n",
        "        self._all_items = set([i for i in range(1, self.num_items + 1)])\n",
        "\n",
        "        # rng\n",
        "        if random_seed is None:\n",
        "            self.rng = random.Random()\n",
        "        else:\n",
        "            self.rng = random.Random(random_seed)\n",
        "\n",
        "        # tokens\n",
        "        self.mask_token = self.num_items + 1\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # 총 user 수(학습에 사용할 sequence 수수)\n",
        "        return self.num_users\n",
        "\n",
        "    def __getitem__(self, user):\n",
        "        seq = self.user_train[user]\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for s in seq[-self.sequence_len:]:\n",
        "            prob = np.random.random()\n",
        "            if prob < self.mask_prob:\n",
        "                prob /= self.mask_prob\n",
        "                # BERT 학습\n",
        "                # random 하게 80% 를 mask token 으로 변환\n",
        "                if prob < 0.8:\n",
        "                    # masking\n",
        "                    # mask_index : num_item + 1 , 0 : pad, 1 ~ num_item : item index\n",
        "                    tokens.append(self.mask_token)\n",
        "                elif prob < 0.9:\n",
        "                    # item random sampling (noise)\n",
        "                    tokens.append(np.random.randint(1, self.num_items))\n",
        "                else:\n",
        "                    # 나머지 10% 를 original token 으로 사용\n",
        "                    tokens.append(s)\n",
        "                labels.append(s) # 학습에 사용\n",
        "            # training 에 사용하지 않음 \n",
        "            else:\n",
        "                tokens.append(s)\n",
        "                labels.append(0) # 학습에 사용하지 않음, trivial\n",
        "\n",
        "        # zero padding \n",
        "        # tokens 와 labels 가 padding_len 보다 짧으면 zero padding 을 해준다. \n",
        "        padding_len = self.sequence_len - len(tokens)\n",
        "        \n",
        "        tokens = [0] * padding_len + tokens\n",
        "        labels = [0] * padding_len + labels\n",
        "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
        "\n",
        "    def _getseq(self, user):\n",
        "        return self.user_train[user]\n",
        "\n",
        "    def random_neg_sampling(self, sold_items, num_item_sample):\n",
        "        nge_samples = random.sample(list(self._all_items - set(sold_items)), num_item_sample)\n",
        "        return nge_samples\n",
        "\n",
        "class BertEvalDataset(Dataset):\n",
        "    def __init__(self, u2seq, u2answer, sequence_len, mask_token, negative_samples):\n",
        "        self.u2seq = u2seq\n",
        "        self.users = sorted(self.u2seq.keys())\n",
        "        self.u2answer = u2answer\n",
        "        self.sequence_len = sequence_len\n",
        "        self.mask_token = mask_token\n",
        "        self.negative_samples = negative_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user = self.users[index]\n",
        "        seq = self.u2seq[user]\n",
        "        answer = self.u2answer[user]\n",
        "        negs = self.random_neg_sampling(sold_item=user, num_item_sample = 1)\n",
        "\n",
        "        candidates = answer + negs\n",
        "        labels = [1] * len(answer) + [0] * len(negs)\n",
        "\n",
        "        seq = seq + [self.mask_token]\n",
        "        seq = seq[-self.sequence_len:]\n",
        "        padding_len = self.sequence_len - len(seq)\n",
        "        seq = [0] * padding_len + seq\n",
        "\n",
        "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n",
        "    \n",
        "    def random_neg_sampling(self, sold_items, num_item_sample : int):\n",
        "        nge_samples = random.sample(list(self._all_items - set(sold_items)), num_item_sample)\n",
        "        return nge_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN1cYr2MoqOk"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Util"
      ],
      "metadata": {
        "id": "WmG9hPNv0ehl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    cudnn.deterministic = True\n",
        "    cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "MeQuNUuwQOJn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "ihaTaFPBR4Dh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "PV_0pdBnf5wx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    # layer가 많아지면 학습이 잘 안되는 현상\n",
        "    # 따라서, sub-layer들에 각각 dropout을 한 후 서로 residual connection을 적용하고, layer normalization 적용\n",
        "    def __init__(self, hidden_units, dropout):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.norm = LayerNorm(hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, sublayer, x):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        r = self.norm(x)\n",
        "        r = sublayer(r)\n",
        "\n",
        "        r = self.dropout(r)\n",
        "\n",
        "        return x + r"
      ],
      "metadata": {
        "id": "yViBKzGuiXQZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_1 = nn.Linear(hidden_units, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
      ],
      "metadata": {
        "id": "u7zJ2-uOSdnR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "3fXOiUlHq9ck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KaBmHAqnoPbb"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None, dropout=None):\n",
        "        \"\"\"\n",
        "            b = batch, ? = num_heads, L = sequence_len\n",
        "\n",
        "            Q: (b x ? x L x dim_Q)\n",
        "            K: (b x ? x L x dim_K)\n",
        "            V: (b x ? x L x dim_V)\n",
        "            ?: 1 (squeezed) or h (multi-head)\n",
        "            mask: (b x ? x L x L)\n",
        "            dropout: nn.Module\n",
        "            assuming dim_Q = dim_K\n",
        "        \"\"\"\n",
        "\n",
        "        # A: (b x ? x L x L)\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
        "\n",
        "        # apply mask (the logit value of a padding token should be minus infinity)\n",
        "        if mask is not None:\n",
        "            attn_score = attn_score.masked_fill(mask == 0, -1e9) # 유사도가 0인 지점은 -infinity로 보내서 softmax 결과가 0이 아니게 함\n",
        "        \n",
        "        # getting normalized(probability) weights through softmax (when padding token, it'll be 0)\n",
        "        # P: (b x ? x L x L)\n",
        "        p_attn = F.softmax(attn_score, dim=-1)\n",
        "        \n",
        "        # apply dropout (with given dropout)\n",
        "        if dropout is not None:\n",
        "            p_attn = self.dropout(p_attn) # attention distribution 상대적 중요도\n",
        "\n",
        "        # (b x ? x L x L) @ (b x ? x L x dim_V) -> (b x ? x L x dim_V)\n",
        "        output = torch.matmul(p_attn, V)\n",
        "\n",
        "        return output, p_attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout=0.1):\n",
        "\n",
        "        \"\"\"\n",
        "            dim_V should be equal to hidden_units / num_heads\n",
        "            we assume dim_Q = dim_K = dim_V\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        assert hidden_units % num_heads == 0\n",
        "\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.dim_V = hidden_units // num_heads\n",
        "\n",
        "        # query, key, value, output 생성을 위한 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "\n",
        "        self.W_O = nn.Linear(hidden_units * num_heads, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layerNorm = LayerNorm(hidden_units)\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "\n",
        "        residual = enc\n",
        "\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # 1) Do all the linear projections in a batch from dim_model, then split into (num_heads x dim_V)\n",
        "        # [process]\n",
        "        # (1) linear(W): (b x L x dim_model) -> (b x L x dim_model)\n",
        "        # (2) view: (b x L x dim_model) -> (b x L x num_heads x dim_V)\n",
        "        # (3) transpose: (b x L x num_heads x dim_V) -> (b x num_heads x L x dim_V)\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "\n",
        "        # 2) Apply attention to the projected vectors in the batch\n",
        "        # note that attenion only cares about the last two dimensions\n",
        "        # output: (b x num_heads x L x dim_V)\n",
        "        # Head별로 각기 다른 attention이 가능하도록 각각 attention에 통과시킴\n",
        "        output, attn_dist = self.attention(Q, K, V, mask=mask, dropout=self.dropout) # attn_dist : (batch_size, num_heads, sequence_len, sequence_len)\n",
        "\n",
        "        # 3) \"concat\" those heads using view\n",
        "        # [process]\n",
        "        # (1) transpose: (b x num_heads x L x dim_V) -> (b x L x num_heads x dim_V)\n",
        "        # (2) contiguous: reorder memory inside GPU (no dimension change)\n",
        "        # (3) view: (b x L x num_heads x dim_V) -> (b x L x dim_model)\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
        "        output = output.transpose(1, 2).contiguous() # (batch_size, sequence_len, num_heads, d_model) / contiguous() : 가변적 메모리 할당\n",
        "        output = output.view(batch_size, seqlen, -1) # (batch_size, sequence_len, d_model * num_heads)\n",
        "\n",
        "        # 4) apply the final linear\n",
        "        # X: (b x L x dim_model)\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "-Xr1eb1UubmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional Encoder = Transformer (self-attention)\n",
        "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_units=256, num_heads=4, d_ff=1024, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param hidden_units: hidden_units size of transformer\n",
        "        :param num_heads: head sizes of multi-head attention\n",
        "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_units\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, hidden_units=hidden_units, dropout=dropout)\n",
        "        self.attention_sublayer = SublayerConnection(hidden_units=hidden_units, dropout=dropout)\n",
        "        self.pwff = PositionwiseFeedForward(hidden_units=hidden_units, d_ff=d_ff, dropout=dropout)\n",
        "        self.pwff_sublayer = SublayerConnection(hidden_units=hidden_units, dropout=dropout)\n",
        "        self.dropoutlayer = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_enc, mask=None):\n",
        "        # we need dynamic mask for the attention forward (sublayer module also has parameters, namely layernorm)\n",
        "        # x: (b x L x dim_model)\n",
        "        # mask: (b x L x L), set False to ignore that point\n",
        "        x = self.attention_sublayer(lambda _x: self.attention(_x, mask=mask), input_enc)\n",
        "        x = self.pwff_sublayer(self.pwff, x)\n",
        "        return self.dropoutlayer(x)\n",
        "\n",
        "class BERT4Rec(nn.Module):\n",
        "    def __init__(self, num_users, num_items, sequence_len, device, num_layers=2, hidden_units=256, num_heads=4, dropout=0.1, random_seed=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # params\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.sequence_len = sequence_len\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.random_seed = random_seed\n",
        "        self.device = device\n",
        "        \n",
        "        # set seed\n",
        "        if random_seed is not None:\n",
        "            fix_random_seed(random_seed)\n",
        "\n",
        "        # 0: padding token\n",
        "        # 1 ~ V: item tokens\n",
        "        # V + 1: mask token\n",
        "\n",
        "        # Embedding layers\n",
        "        self.item_emb = nn.Embedding(num_items + 2, hidden_units, padding_idx=0) # padding : 0 / item : 1 ~ num_item + 1 /  mask : num_item + 2\n",
        "        self.pos_emb = nn.Embedding(sequence_len, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "\n",
        "        # transformer layers\n",
        "        self.transformers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                hidden_units=hidden_units,\n",
        "                num_heads=num_heads,\n",
        "                d_ff=hidden_units*4,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.out = nn.Linear(hidden_units, num_items + 1)\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        L = tokens.shape[1]\n",
        "\n",
        "        # mask for whether padding token or not in attention matrix (True if padding token)\n",
        "        # [process] (b x L) -> (b x 1 x L) -> (b x L x L) -> (b x 1 x L x L)\n",
        "        token_mask = torch.BoolTensor(tokens > 0).unsqueeze(1).repeat(1, L, 1).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # get embedding\n",
        "        # [process] (b x L) -> (b x L x d)\n",
        "        seqs = self.item_emb(torch.LongTensor(tokens).to(self.device))  # (batch_size, sequence_len, hidden_units)\n",
        "        positions = np.tile(np.array(range(tokens.shape[1])), [tokens.shape[0], 1])  # (batch_size, sequence_len)\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))  # (batch_size, sequence_len, hidden_units)\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs))  # LayerNorm\n",
        "\n",
        "        # apply multi-layered transformers\n",
        "        # [process] (b x L x d) -> ... -> (b x L x d)\n",
        "        for transformer in self.transformers:\n",
        "            seqs = transformer(seqs, token_mask)\n",
        "\n",
        "        # classifier\n",
        "        # [process] (b x L x d) -> (b x L x (V + 1))\n",
        "        out = self.out(seqs)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "xtf_I6wZqtsL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER_MsqHwET7d"
      },
      "source": [
        "# 4. 학습함수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Solver:\n",
        "    def calculate_loss(self, batch):\n",
        "        \n",
        "        self.ce_losser = CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        # device\n",
        "        tokens = batch['tokens'].to(self.device)  # b x L\n",
        "        labels = batch['labels'].to(self.device)  # b x L\n",
        "\n",
        "        # forward\n",
        "        logits = self.model(tokens)  # b x L x (V + 1)\n",
        "\n",
        "        # loss\n",
        "        logits = logits.view(-1, logits.size(-1))  # bL x (V + 1)\n",
        "        labels = labels.view(-1)  # bL\n",
        "        loss = self.ce_losser(logits, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def set_model_mode(self, purpose: str) -> None:\n",
        "        if purpose == 'eval':\n",
        "            self.model = self.model.eval()  # type: ignore\n",
        "        elif purpose == 'train':\n",
        "            self.model = self.model.train()\n",
        "\n",
        "    def solve(self) -> None:\n",
        "        C = self.config\n",
        "        name = C['name']\n",
        "        print(f\"solve {name} start\")\n",
        "\n",
        "        # report new session\n",
        "        self.logger.info(\"-- new solve session started --\")\n",
        "\n",
        "        # report model parameters\n",
        "        numels = []\n",
        "        for parameter in self.model.parameters():\n",
        "            if parameter.requires_grad:\n",
        "                numels.append(parameter.numel())\n",
        "        num_params = sum(numels)\n",
        "        self.logger.info(f\"total parameters:\\t{num_params}\")\n",
        "        with open(os.path.join(self.data_dir, 'meta.json'), 'w') as fp:\n",
        "            json.dump({'num_params': num_params}, fp)\n",
        "\n",
        "        # solve loop\n",
        "        self.early_stop = False\n",
        "        self.patience_counter = 0\n",
        "        self.load_model('train')\n",
        "        for epoch in range(self.start_epoch, C['train']['epoch'] + 1):\n",
        "            self.solve_train(epoch)\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "            if self.early_stop:\n",
        "                break\n",
        "\n",
        "        # solve end\n",
        "        self.load_model('test')\n",
        "        self.solve_test()\n",
        "\n",
        "    def solve_train(self, epoch: int) -> None:\n",
        "            self.train_one_epoch(epoch)\n",
        "            self.evaluate_on_valid(epoch)\n",
        "\n",
        "    def train_one_epoch(self, epoch: int) -> None:\n",
        "        # prepare\n",
        "        self.set_model_mode('train')\n",
        "        epoch_loss_sum = 0.0\n",
        "        epoch_loss_count = 0\n",
        "\n",
        "        # loop\n",
        "        pbar = tqdm(self.train_dataloader)\n",
        "        pbar.set_description(f\"[epoch {epoch} train]\")\n",
        "        for batch in pbar:\n",
        "\n",
        "            # get loss\n",
        "            loss = self.calculate_loss(batch)\n",
        "\n",
        "            # backward\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # step end\n",
        "            epoch_loss_sum += float(loss.data)\n",
        "            epoch_loss_count += 1\n",
        "            pbar.set_postfix(loss=f'{epoch_loss_sum / epoch_loss_count:.4f}')\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        # epoch end\n",
        "        epoch_loss = epoch_loss_sum / epoch_loss_count\n",
        "        self.logger.info('\\t'.join([\n",
        "            f\"[epoch {epoch}]\",\n",
        "            f\"train loss: {epoch_loss:.4f}\"\n",
        "        ]))\n",
        "        self.writer.add_scalar('train_loss', epoch_loss, epoch)\n",
        "\n",
        "    def evaluate_on_valid(self, epoch: int) -> None:\n",
        "        self.logger.info(f\"<evaluate on valid (epoch: {epoch})>\")\n",
        "        C = self.config\n",
        "\n",
        "        # prepare\n",
        "        self.set_model_mode('eval')\n",
        "        ks = sorted(C['metric']['ks'])\n",
        "        pivot = C['metric']['pivot']\n",
        "\n",
        "        # init\n",
        "        result_values = []\n",
        "\n",
        "        # loop\n",
        "        pbar = tqdm(self.valid_dataloader)\n",
        "        pbar.set_description(f\"[epoch {epoch} valid]\")\n",
        "        with torch.no_grad():\n",
        "            for batch in pbar:\n",
        "\n",
        "                # get rankers\n",
        "                rankers = self.calculate_rankers(batch)\n",
        "\n",
        "                # evaluate\n",
        "                labels = batch['labels'].to(self.device)\n",
        "                batch_results = calc_batch_rec_metrics_per_k(rankers, labels, ks)  # type: ignore\n",
        "                result_values.extend(batch_results['values'][pivot])\n",
        "\n",
        "                # verbose\n",
        "                pbar.set_postfix(pivot_score=f'{sum(result_values) / len(result_values):.4f}')\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        # report\n",
        "        pivot_score = sum(result_values) / len(result_values)\n",
        "        self.logger.info('\\t'.join([\n",
        "            f\"[epoch {epoch}]\",\n",
        "            f\"valid pivot score: {pivot_score:.4f}\"\n",
        "        ]))\n",
        "        self.writer.add_scalar('valid_pivot_score', pivot_score, epoch)\n",
        "\n",
        "        # save best\n",
        "        if self.best_score is None or self.best_score < pivot_score:\n",
        "            self.best_score = pivot_score\n",
        "            torch.save(self.model.state_dict(), self.model_path)\n",
        "            self.logger.info(f\"updated best model at epoch {epoch} with pivot score {pivot_score}\")\n",
        "            self.patience_counter = 0\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "        if self.patience_counter >= C['train']['patience']:\n",
        "            self.logger.info(f\"no update for {self.patience_counter} epochs, thus early stopping\")\n",
        "            self.early_stop = True\n",
        "\n",
        "        # save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'best_score': self.best_score,\n",
        "            'model': self.model.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "        }, self.check_path)"
      ],
      "metadata": {
        "id": "X1hDGNZsBofe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "def train_one_epoch(epoch, train_dataloader):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss_sum = 0.0\n",
        "    epoch_loss_count = 0\n",
        "\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    pbar.set_description(f\"[epoch {epoch} train\")\n",
        "    \n",
        "    for batch in pbar:\n",
        "        loss = calculate_loss(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss_sum += float(loss.data)\n",
        "        epoch_loss_count += 1\n",
        "        pbar.set_postfix(loss=f'{epoch_loss_sum / epoch_loss_count:.4f}')\n",
        "    \n",
        "    pbar.close()\n",
        "\n",
        "    epoch_loss = epoch_loss_sum / epoch_loss_count\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "def evaluate_on_valid(epoch, valid_dataloader):\n",
        "    model.eval()\n",
        "    \n",
        "    NDCG = 0.0\n",
        "    HIT = 0.0\n",
        "\n",
        "    num_item_sample = 100\n",
        "    users = [user for user in range(make_sequence_dataset.num_user)]\n",
        "    \n",
        "    result_values = []\n",
        "\n",
        "    pbar = tqdm(valid_dataloader)\n",
        "    pbar.set_description(f\"[epoch {epoch} valid\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            rankers = calculate_rankers(batch)\n",
        "\n",
        "            labels = batch['labels'].to(config['device'])\n",
        "            \n",
        "\n",
        "\n",
        "def train(model, criterion, optimizer, data_loader):\n",
        "    \n",
        "    for epoch in range(1, config['num_epochs'] + 1):\n",
        "        train_loss = train_one_epoch(epoch,data_loader)\n",
        "\n",
        "\n",
        "\n",
        "    for seq, labels in data_loader:\n",
        "        logits = model(seq)\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        labels = labels.view(-1).to(config['device'])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def evaluate(model, user_train, user_valid, max_len, bert4rec_dataset, make_sequence_dataset):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10\n",
        "    HIT = 0.0 # HIT@10\n",
        "\n",
        "    num_item_sample = 100\n",
        "\n",
        "    users = [user for user in range(make_sequence_dataset.num_user)]\n",
        "\n",
        "    for user in users:\n",
        "        seq = (user_train[user] + [make_sequence_dataset.num_item + 1])[-max_len:]\n",
        "        rated = user_train[user] + user_valid[user]\n",
        "        items = user_valid[user] + bert4rec_dataset.random_neg_sampling(rated_item = rated, num_item_sample = num_item_sample)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions = -model(np.array([seq]))\n",
        "            predictions = predictions[0][-1][items] # sampling\n",
        "            rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        if rank < 10: #Top10\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HIT += 1\n",
        "\n",
        "    NDCG /= len(users)\n",
        "    HIT /= len(users)\n",
        "\n",
        "    return NDCG, HIT\n",
        "\n",
        "def calculate_loss(batch):\n",
        "        \n",
        "    ce_losser = CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    # device\n",
        "    tokens = batch['tokens'].to(config['device'])  # b x L\n",
        "    labels = batch['labels'].to(config['device'])  # b x L\n",
        "\n",
        "    # forward\n",
        "    logits = model(tokens)  # b x L x (V + 1)\n",
        "\n",
        "    # loss\n",
        "    logits = logits.view(-1, logits.size(-1))  # bL x (V + 1)\n",
        "    labels = labels.view(-1)  # bL\n",
        "    loss = ce_losser(logits, labels)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def calculate_rankers(batch):\n",
        "\n",
        "    # device\n",
        "    tokens = batch['tokens'].to(config['device'])  # b x L\n",
        "    cands = batch['cands'].to(config['device'])  # b x C\n",
        "\n",
        "    # forward\n",
        "    logits = model(tokens)  # b x L x (V + 1)\n",
        "\n",
        "    # gather\n",
        "    logits = logits[:, -1, :]  # b x (V + 1)\n",
        "    scores = logits.gather(1, cands)  # b x C\n",
        "    rankers = scores.argsort(dim=1, descending=True)\n",
        "\n",
        "    return rankers"
      ],
      "metadata": {
        "id": "Jf2zw6CXzM6o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(scores, labels, k):\n",
        "    scores = scores.cpu()\n",
        "    labels = labels.cpu()\n",
        "    rank = (-scores).argsort(dim=1)\n",
        "    cut = rank[:, :k]\n",
        "    hit = labels.gather(1, cut)\n",
        "    return (hit.sum(1).float() / labels.sum(1).float()).mean().item()\n",
        "\n",
        "\n",
        "def ndcg(scores, labels, k):\n",
        "    scores = scores.cpu()\n",
        "    labels = labels.cpu()\n",
        "    rank = (-scores).argsort(dim=1)\n",
        "    cut = rank[:, :k]\n",
        "    hits = labels.gather(1, cut)\n",
        "    position = torch.arange(2, 2+k)\n",
        "    weights = 1 / torch.log2(position.float())\n",
        "    dcg = (hits.float() * weights).sum(1)\n",
        "    idcg = torch.Tensor([weights[:min(n, k)].sum() for n in labels.sum(1)])\n",
        "    ndcg = dcg / idcg\n",
        "    return ndcg.mean()\n",
        "\n",
        "\n",
        "def recalls_and_ndcgs_for_ks(scores, labels, ks):\n",
        "    metrics = {}\n",
        "\n",
        "    scores = scores.cpu()\n",
        "    labels = labels.cpu()\n",
        "    answer_count = labels.sum(1)\n",
        "    answer_count_float = answer_count.float()\n",
        "    labels_float = labels.float()\n",
        "    rank = (-scores).argsort(dim=1)\n",
        "    cut = rank\n",
        "    for k in sorted(ks, reverse=True):\n",
        "       cut = cut[:, :k]\n",
        "       hits = labels_float.gather(1, cut)\n",
        "       metrics['Recall@%d' % k] = (hits.sum(1) / answer_count_float).mean().item()\n",
        "\n",
        "       position = torch.arange(2, 2+k)\n",
        "       weights = 1 / torch.log2(position.float())\n",
        "       dcg = (hits * weights).sum(1)\n",
        "       idcg = torch.Tensor([weights[:min(n, k)].sum() for n in answer_count])\n",
        "       ndcg = (dcg / idcg).mean()\n",
        "       metrics['NDCG@%d' % k] = ndcg\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "W6TvWguEj-PY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxqJK--xKgyb"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "63fbu_6GKbbV"
      },
      "outputs": [],
      "source": [
        "make_sequence_dataset = MakeSequenceDataSet(config=config)\n",
        "user_train, user_valid = make_sequence_dataset.get_train_valid_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rpe4KS4qWybY"
      },
      "outputs": [],
      "source": [
        "bert4rec_dataset = BERTTrainDataSet(\n",
        "    user_train = user_train, \n",
        "    sequence_len = config.sequence_len, \n",
        "    num_users = make_sequence_dataset.num_users, \n",
        "    num_items = make_sequence_dataset.num_items,\n",
        "    mask_prob = config.mask_prob,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gIIjMMm7XR_v"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(\n",
        "    bert4rec_dataset, \n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = config.num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "37s2MFEqXZil"
      },
      "outputs": [],
      "source": [
        "model = BERT4Rec(\n",
        "    num_users = make_sequence_dataset.num_users, \n",
        "    num_items = make_sequence_dataset.num_items, \n",
        "    hidden_units = config.hidden_units, \n",
        "    num_heads = config.num_heads, \n",
        "    num_layers = config.num_layers, \n",
        "    sequence_len = config.sequence_len, \n",
        "    dropout = config.dropout, \n",
        "    device = device,\n",
        "    ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 2\n",
        "for seq, lab in data_loader:\n",
        "    pred = model(seq)\n",
        "    print(seq, pred[i], lab, sep='\\n')\n",
        "    break"
      ],
      "metadata": {
        "id": "eQljaPfwfnYU",
        "outputId": "08f7eddf-87c2-432b-d640-182b30080196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[44447, 44447, 18548,  ..., 44447, 16537, 44447],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ..., 28533,   959,  5417],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,     0,     0, 44447],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0]])\n",
            "tensor([[3.1449e-01, -2.8720e+00, -3.8971e-01,  ..., 1.3883e+00, 2.2048e+00, 6.8758e-01],\n",
            "        [1.6308e+00, -1.4954e+00, 4.3418e-01,  ..., -4.5366e-01, 3.5201e+00, 2.5185e+00],\n",
            "        [-2.6254e+00, -1.0772e+00, 2.4350e+00,  ..., 2.3036e-01, 1.1446e+00, 1.9856e+00],\n",
            "        ...,\n",
            "        [1.5002e+00, 6.1379e+00, -2.6798e+00,  ..., -5.9342e+00, -2.6394e+00, 7.8676e-01],\n",
            "        [1.5422e+00, 1.2590e+00, 2.6335e+00,  ..., 1.2225e+00, -2.7707e-01, -1.8416e+00],\n",
            "        [3.4384e+00, -7.1496e-01, 1.6637e+00,  ..., -3.9029e+00, -3.0832e-01, 8.2866e-01]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor([[43539, 18462,     0,  ..., 37323,     0, 16851],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,     0,     0,  1855],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(seq)"
      ],
      "metadata": {
        "id": "pWvCAaHjiIkw",
        "outputId": "2c12903a-1661-4189-cef5-4d0f91e2a13f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pred.view(-1, pred.size(-1))))"
      ],
      "metadata": {
        "id": "iPguICJLgloX",
        "outputId": "67a938e1-a81d-4f54-bdf0-a34f008cc90b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.view(-1, pred.size(-1))"
      ],
      "metadata": {
        "id": "oE8y31kJhRw9",
        "outputId": "a047842f-cf7d-425b-a5bd-9efdf738e6e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-7.2570e-01, 1.0700e+00, -2.6984e+00,  ..., -1.3614e+00, -1.5956e+00, 5.4881e-01],\n",
              "        [-1.4647e-01, 4.3533e-01, -1.1381e+00,  ..., -4.5181e+00, -3.7112e-01, 2.2104e+00],\n",
              "        [-2.8340e+00, 4.7955e-01, -3.4024e-02,  ..., -2.0030e+00, -2.9290e-02, 1.0640e+00],\n",
              "        ...,\n",
              "        [4.0937e+00, 6.3559e-01, -1.3037e+00,  ..., -2.4323e+00, -6.0031e-01, 2.4879e-01],\n",
              "        [7.1378e-01, 1.4318e+00, -2.5132e+00,  ..., -1.1589e+00, -2.4009e+00, -2.3700e+00],\n",
              "        [-1.4364e+00, 4.4246e-01, 4.6346e-02,  ..., -9.3367e-01, -1.0362e+00, -2.6605e-01]],\n",
              "       device='cuda:0', grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(lab.view(-1)))"
      ],
      "metadata": {
        "id": "NFZymejshzjw",
        "outputId": "d6a238d0-7644-4f47-fda9-d4cc43c33481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.view(-1)"
      ],
      "metadata": {
        "id": "sh03Sdngh7Vo",
        "outputId": "bb803d8d-51bf-4606-c7f8-d99b8d5d94f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,     0,     0,  ...,     0, 35168,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, data_loader):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for seq, labels in data_loader:\n",
        "        logits = model(seq)  # B x T x V\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))  # (B*T) x V\n",
        "        labels = labels.view(-1).to(device)  # B*T\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def evaluate(model, user_train, user_valid, sequence_len, bert4rec_dataset, make_sequence_dataset):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10\n",
        "    HIT = 0.0 # HIT@10\n",
        "\n",
        "    num_item_sample = 100\n",
        "\n",
        "    users = [user for user in range(make_sequence_dataset.num_users)]\n",
        "\n",
        "    for user in users:\n",
        "        seq = (user_train[user] + [make_sequence_dataset.num_items + 1] * config['val_data'])[-sequence_len:]\n",
        "        sold = user_train[user] + user_valid[user]\n",
        "        items = user_valid[user] + bert4rec_dataset.random_neg_sampling(sold_items = sold, num_item_sample = num_item_sample)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions = model(np.array([seq]))\n",
        "            predictions = predictions[0][-1][items] # sampling\n",
        "            rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        if rank < 10: #Top10\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HIT += 1\n",
        "\n",
        "    NDCG /= len(users)\n",
        "    HIT /= len(users)\n",
        "\n",
        "    return NDCG, HIT"
      ],
      "metadata": {
        "id": "ETNnkwbl03H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjSWoKLmXa45",
        "outputId": "be7b823d-003e-4bdb-853b-ab4890466340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 10.40876| NDCG@10: 0.04494| HIT@10: 0.09829:   9%|▊         | 17/199 [1:09:27<12:17:21, 243.09s/it]"
          ]
        }
      ],
      "source": [
        "loss_list = []\n",
        "ndcg_list = []\n",
        "hit_list = []\n",
        "for epoch in range(1, config['num_epochs'] + 1):\n",
        "    model.train()\n",
        "    tbar = tqdm(data_loader)\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            data_loader = data_loader)\n",
        "        \n",
        "        ndcg, hit = evaluate(\n",
        "            model = model, \n",
        "            user_train = user_train, \n",
        "            user_valid = user_valid, \n",
        "            sequence_len = config.sequence_len,\n",
        "            bert4rec_dataset = bert4rec_dataset, \n",
        "            make_sequence_dataset = make_sequence_dataset,\n",
        "            )\n",
        "\n",
        "        loss_list.append(train_loss)\n",
        "        ndcg_list.append(ndcg)\n",
        "        hit_list.append(hit)\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IYFTapfXcvb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
        "ax = ax.flatten()\n",
        "epochs = [i for i in range(1, config.num_epochs + 1)]\n",
        "\n",
        "ax[0].plot(epochs, loss_list)\n",
        "ax[0].set_title('Loss')\n",
        "\n",
        "ax[1].plot(epochs, ndcg_list)\n",
        "ax[1].set_title('NDCG')\n",
        "\n",
        "ax[2].plot(epochs, hit_list)\n",
        "ax[2].set_title('HIT')\n",
        "plt.show()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRRUv2GcwM_I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGs9cfpY74lsjnesuQ74sB",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}