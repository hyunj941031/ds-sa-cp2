{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunj941031/ds-sa-cp2/blob/main/models/BERT4Rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vg0sx3_eDd5",
        "outputId": "8d8b7a86-43c1-47f5-c4ae-0290d51d9dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IatRo2Phd6_n",
        "outputId": "beb243bc-a328-421e-9c79-a3c012f6c735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-box in /usr/local/lib/python3.8/dist-packages (7.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zaXMbpoEeB03"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "torch.set_printoptions(sci_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls46hifUgmt8"
      },
      "source": [
        "# 1. 학습 parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "kQr30WC2eQ8E"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : '/content/drive/MyDrive/fashion_campus_dataset',\n",
        "    'max_len' : 50,\n",
        "    'hidden_units' : 500, # Embedding size\n",
        "    'num_heads' : 2, # Multi-head layer 수 (병렬처리)\n",
        "    'num_layers' : 3, # block 수 (encoder layer 수)\n",
        "    'dropout_rate' : 0.4, # dropout의 비율\n",
        "    'lr' : 0.001,\n",
        "    'batch_size' : 128,\n",
        "    'num_epochs' : 10,\n",
        "    'num_workers' : 4,\n",
        "    'mask_prob' : 0.8, # cloze task\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsuqMA2_gqXH"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "pwHqzLnkgf4I"
      },
      "outputs": [],
      "source": [
        "class MakeSequenceDataSet():\n",
        "    \"\"\"\n",
        "    SequenceData 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'user_item.csv'))\n",
        "\n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('itemId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['itemId'].apply(lambda x : self.item_encoder[x] + 1)\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "        self.df = self.df.sort_values(['user_idx', 'timestamp']) # 시간에 따른 정렬\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col:str) -> dict:\n",
        "        '''\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Return:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        '''\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "        \n",
        "        return encoder, decoder\n",
        "\n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        '''\n",
        "        sequence data 생성\n",
        "\n",
        "        Return:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        '''\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        group_df = self.df.groupby('user_idx')\n",
        "        for user, item in group_df:\n",
        "            users[user].extend(item['item_idx'].tolist())\n",
        "\n",
        "        for user in users:\n",
        "            user_train[user] = users[user][:-1]\n",
        "            user_valid[user] = [users[user][-1]] # 마지막 아이템 예측\n",
        "\n",
        "        return user_train, user_valid\n",
        "\n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "iZf_2Zq9k4ix"
      },
      "outputs": [],
      "source": [
        "class BERTRecDataSet(Dataset):\n",
        "    def __init__(self, user_train, max_len, num_user, num_item, mask_prob):\n",
        "        self.user_train = user_train\n",
        "        self.max_len = max_len\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.mask_prob = mask_prob\n",
        "        self._all_items = set([i for i in range(1, self.num_item + 1)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_user\n",
        "\n",
        "    def __getitem__(self, user):\n",
        "        user_seq = self.user_train[user]\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for s in user_seq[-self.max_len:]:\n",
        "            prob = np.random.random()\n",
        "            if prob < self.mask_prob:\n",
        "                prob /= self.mask_prob\n",
        "                if prob < 0.8:\n",
        "                    # masking\n",
        "                    tokens.append(self.num_item + 1) # mask_index: num_item + 1, 0: pad, 1~num_item: item index\n",
        "                elif prob < 0.9:\n",
        "                    # noise를 위한 neg item sampling\n",
        "                    tokens.extend(self.random_neg_sampling(rated_item = user_seq, num_item_sample = 1)) # item random sampling\n",
        "                else:\n",
        "                    tokens.append(s)\n",
        "                labels.append(s) # 학습에 사용\n",
        "            else:\n",
        "                tokens.append(s)\n",
        "                labels.append(0) # 학습에 사용 x\n",
        "                \n",
        "        mask_len = self.max_len - len(tokens)\n",
        "        tokens = [0] * mask_len + tokens\n",
        "        labels = [0] * mask_len + labels\n",
        "\n",
        "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
        "\n",
        "    def random_neg_sampling(self, rated_item, num_item_sample : int):\n",
        "        nge_samples = random.sample(list(self._all_items - set(rated_item)), num_item_sample)\n",
        "        return nge_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN1cYr2MoqOk"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "KaBmHAqnoPbb"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        '''\n",
        "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
        "        mask : (batch_size, 1, max_len, max_len)\n",
        "        '''\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
        "        attn_score = attn_score. masked_fill(mask == 0, -1e9) # 유사도가 0인 지점은 -infinity로 보내서 softmax 결과가 0이 아니게 함\n",
        "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1)) # attention distribution\n",
        "        output = torch.matmul(attn_dist, V)\n",
        "        return output, attn_dist\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "        # query, key value, output 생성을 위한 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units * num_heads, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6)\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "        '''\n",
        "        enc : (batch_size, max_len, hidden_units)\n",
        "        mask : (batch_size, 1, max_len, max_len)\n",
        "        '''\n",
        "        residual = enc # residual connection을 위한 residual 저장\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "\n",
        "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
        "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
        "\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
        "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
        "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
        "\n",
        "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
        "        return output, attn_dist\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "\n",
        "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
        "        output = self.layerNorm(self.dropout(output) + residual)\n",
        "        return output\n",
        "\n",
        "class BERT4RecBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(BERT4RecBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
        "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
        "\n",
        "    def forward(self, input_enc, mask):\n",
        "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
        "        output_enc = self.pointwise_feedforward(output_enc)\n",
        "        return output_enc, attn_dist\n",
        "\n",
        "class BERT4Rec(nn.Module):\n",
        "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device):\n",
        "        super(BERT4Rec, self).__init__()\n",
        "\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers \n",
        "        self.device = device\n",
        "        \n",
        "        self.item_emb = nn.Embedding(num_item + 2, hidden_units, padding_idx=0) # padding : 0 / item : 1 ~ num_item + 1 /  mask : num_item + 2\n",
        "        self.pos_emb = nn.Embedding(max_len, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([BERT4RecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
        "        self.out = nn.Linear(hidden_units, num_item + 1)\n",
        "    \n",
        "    def forward(self, log_seqs):\n",
        "        \"\"\"\n",
        "        log_seqs : (batch_size, max_len)\n",
        "\n",
        "        ex)\n",
        "        log_seqs = [\n",
        "                [1, 2, 3, 4, 5],\n",
        "                [0, 0, 0, 1, 2],\n",
        "                [0, 0, 1, 2, 3]\n",
        "        ]\n",
        "        \n",
        "        \"\"\"\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device)) # (batch_size, max_len, hidden_units)\n",
        "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1]) # (batch_size, max_len)\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device)) # (batch_size, max_len, hidden_units)\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs)) # LayerNorm\n",
        "\n",
        "        mask_pad = torch.BoolTensor(log_seqs > 0).unsqueeze(1).repeat(1, log_seqs.shape[1], 1).unsqueeze(1).to(self.device) # mask for zero pad / (batch_size, 1, max_len, max_len)\n",
        "        for block in self.blocks:\n",
        "            seqs, attn_dist = block(seqs, mask_pad)\n",
        "        out = self.out(seqs) # (batch_size, max_len, num_item + 1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER_MsqHwET7d"
      },
      "source": [
        "# 4. 학습함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "TXrvDfLKqkWp"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, data_loader):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for seq, labels in data_loader:\n",
        "        logits = model(seq)\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        labels = labels.view(-1).to(device) # 같은 device에 들어가게끔\n",
        "\n",
        "        optimizer.zero_grad() # 역전파 시, 루프마다 간섭을 하지 않게 하기 위해서 grad값들을 0으로 초기화\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def evaluate(model, user_train, user_valid, max_len, bert4rec_dataset, make_sequence_dataset):\n",
        "    model.eval() # layer들의 동작을 inference(evaluation) mode로 바꿔주는 목적 -> BatchNorm 등\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10 / normalized discounted cumulative gain : 랭킹 결과의 길이에 상관없이 일정한 스케일을 갖도록 하는 normalize\n",
        "    HIT = 0.0 # HIT@10 / 전체 사용자 수 대비 적중한 사용자 수\n",
        "\n",
        "    num_item_sample = 100\n",
        "\n",
        "    users = [user for user in range(make_sequence_dataset.num_user)]\n",
        "\n",
        "    for user in users:\n",
        "        seq = (user_train[user] + [make_sequence_dataset.num_item + 1])[-max_len:]\n",
        "        rated = user_train[user] + user_valid[user]\n",
        "        items = user_valid[user] + bert4rec_dataset.random_neg_sampling(rated_item = rated, num_item_sample = num_item_sample)\n",
        "\n",
        "        with torch.no_grad(): # gradient를 트래킹하지 않음, 메모리 사용량을 줄이고 연산속도 향상\n",
        "            predictions = -model(np.array([seq]))\n",
        "            predictions = predictions[0][-1][items]\n",
        "            rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        if rank < 10: # Top10\n",
        "            NDCG += 1 / np.log2(rank+2)\n",
        "            HIT += 1\n",
        "\n",
        "    NDCG /= len(users)\n",
        "    HIT /= len(users)\n",
        "\n",
        "    return NDCG, HIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxqJK--xKgyb"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "63fbu_6GKbbV"
      },
      "outputs": [],
      "source": [
        "make_sequence_dataset = MakeSequenceDataSet(config=config)\n",
        "user_train, user_valid = make_sequence_dataset.get_train_valid_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "rpe4KS4qWybY"
      },
      "outputs": [],
      "source": [
        "bert4rec_dataset = BERTRecDataSet(\n",
        "    user_train = user_train, \n",
        "    max_len = config.max_len, \n",
        "    num_user = make_sequence_dataset.num_user, \n",
        "    num_item = make_sequence_dataset.num_item,\n",
        "    mask_prob = config.mask_prob,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "gIIjMMm7XR_v"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(\n",
        "    bert4rec_dataset, \n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = config.num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "37s2MFEqXZil"
      },
      "outputs": [],
      "source": [
        "model = BERT4Rec(\n",
        "    num_user = make_sequence_dataset.num_user, \n",
        "    num_item = make_sequence_dataset.num_item, \n",
        "    hidden_units = config.hidden_units, \n",
        "    num_heads = config.num_heads, \n",
        "    num_layers = config.num_layers, \n",
        "    max_len = config.max_len, \n",
        "    dropout_rate = config.dropout_rate, \n",
        "    device = device,\n",
        "    ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FjSWoKLmXa45",
        "outputId": "4d721c15-4075-47f7-9db7-3bb09d06b812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 10.96296| NDCG@10: 0.04454| HIT@10: 0.09816: 100%|██████████| 1/1 [06:17<00:00, 377.46s/it]\n",
            "Epoch:   2| Train loss: 10.82031| NDCG@10: 0.04511| HIT@10: 0.10007: 100%|██████████| 1/1 [06:18<00:00, 378.61s/it]\n",
            "Epoch:   3| Train loss: 10.71357| NDCG@10: 0.04569| HIT@10: 0.10062: 100%|██████████| 1/1 [06:23<00:00, 383.36s/it]\n",
            "Epoch:   4| Train loss: 10.70689| NDCG@10: 0.04534| HIT@10: 0.10029: 100%|██████████| 1/1 [06:22<00:00, 382.61s/it]\n",
            "Epoch:   5| Train loss: 10.70294| NDCG@10: 0.04584| HIT@10: 0.10149: 100%|██████████| 1/1 [06:19<00:00, 379.61s/it]\n",
            "Epoch:   6| Train loss: 10.70022| NDCG@10: 0.04587| HIT@10: 0.09977: 100%|██████████| 1/1 [06:22<00:00, 382.71s/it]\n",
            "Epoch:   7| Train loss: 10.69815| NDCG@10: 0.04554| HIT@10: 0.10011: 100%|██████████| 1/1 [06:14<00:00, 374.56s/it]\n",
            "Epoch:   8| Train loss: 10.69678| NDCG@10: 0.04585| HIT@10: 0.10129: 100%|██████████| 1/1 [06:13<00:00, 373.08s/it]\n",
            "Epoch:   9| Train loss: 10.69473| NDCG@10: 0.04634| HIT@10: 0.10151: 100%|██████████| 1/1 [06:17<00:00, 377.14s/it]\n",
            "Epoch:  10| Train loss: 10.69424| NDCG@10: 0.04540| HIT@10: 0.09898: 100%|██████████| 1/1 [06:17<00:00, 377.83s/it]\n",
            "Epoch:  11| Train loss: 10.69232| NDCG@10: 0.04584| HIT@10: 0.09997: 100%|██████████| 1/1 [06:13<00:00, 373.07s/it]\n",
            "Epoch:  12| Train loss: 10.69140| NDCG@10: 0.04497| HIT@10: 0.09861: 100%|██████████| 1/1 [06:11<00:00, 371.84s/it]\n",
            "Epoch:  13| Train loss: 10.69011| NDCG@10: 0.04557| HIT@10: 0.10025: 100%|██████████| 1/1 [06:10<00:00, 370.35s/it]\n",
            "Epoch:  14| Train loss: 10.69042| NDCG@10: 0.04632| HIT@10: 0.10151: 100%|██████████| 1/1 [06:12<00:00, 372.22s/it]\n",
            "Epoch:  15| Train loss: 10.68911| NDCG@10: 0.04560| HIT@10: 0.10044: 100%|██████████| 1/1 [06:12<00:00, 372.15s/it]\n",
            "Epoch:  16| Train loss: 10.68892| NDCG@10: 0.04528| HIT@10: 0.10009: 100%|██████████| 1/1 [06:16<00:00, 376.42s/it]\n",
            "Epoch:  17| Train loss: 10.68739| NDCG@10: 0.04612| HIT@10: 0.10111: 100%|██████████| 1/1 [06:14<00:00, 374.24s/it]\n",
            "Epoch:  18| Train loss: 10.68700| NDCG@10: 0.04574| HIT@10: 0.10064: 100%|██████████| 1/1 [06:12<00:00, 372.85s/it]\n",
            "Epoch:  19| Train loss: 10.68692| NDCG@10: 0.04497| HIT@10: 0.09896: 100%|██████████| 1/1 [06:12<00:00, 372.45s/it]\n",
            "Epoch:  20| Train loss: 10.68621| NDCG@10: 0.04552| HIT@10: 0.10033: 100%|██████████| 1/1 [06:17<00:00, 377.04s/it]\n",
            "Epoch:  21| Train loss: 10.68557| NDCG@10: 0.04555| HIT@10: 0.09948: 100%|██████████| 1/1 [06:15<00:00, 375.80s/it]\n",
            "Epoch:  22| Train loss: 10.68434| NDCG@10: 0.04463| HIT@10: 0.09814: 100%|██████████| 1/1 [06:15<00:00, 375.40s/it]\n",
            "Epoch:  23| Train loss: 10.68565| NDCG@10: 0.04508| HIT@10: 0.09952: 100%|██████████| 1/1 [06:17<00:00, 377.18s/it]\n",
            "Epoch:  24| Train loss: 10.68388| NDCG@10: 0.04478| HIT@10: 0.09894: 100%|██████████| 1/1 [06:14<00:00, 374.78s/it]\n",
            "Epoch:  25| Train loss: 10.68452| NDCG@10: 0.04473| HIT@10: 0.09997: 100%|██████████| 1/1 [06:16<00:00, 376.03s/it]\n",
            "Epoch:  26| Train loss: 10.68377| NDCG@10: 0.04489| HIT@10: 0.09900: 100%|██████████| 1/1 [06:14<00:00, 374.53s/it]\n",
            "Epoch:  27| Train loss: 10.68391| NDCG@10: 0.04524| HIT@10: 0.10074: 100%|██████████| 1/1 [06:14<00:00, 374.38s/it]\n",
            "Epoch:  28| Train loss: 10.68335| NDCG@10: 0.04520| HIT@10: 0.09993: 100%|██████████| 1/1 [06:16<00:00, 376.31s/it]\n",
            "Epoch:  29| Train loss: 10.68337| NDCG@10: 0.04517| HIT@10: 0.10025: 100%|██████████| 1/1 [06:14<00:00, 374.28s/it]\n",
            "Epoch:  30| Train loss: 10.68341| NDCG@10: 0.04535| HIT@10: 0.09973: 100%|██████████| 1/1 [06:16<00:00, 376.17s/it]\n",
            "Epoch:  31| Train loss: 10.68178| NDCG@10: 0.04536| HIT@10: 0.10009: 100%|██████████| 1/1 [06:15<00:00, 375.32s/it]\n",
            "Epoch:  32| Train loss: 10.68301| NDCG@10: 0.04489| HIT@10: 0.09946: 100%|██████████| 1/1 [06:17<00:00, 377.39s/it]\n",
            "Epoch:  33| Train loss: 10.68282| NDCG@10: 0.04490| HIT@10: 0.09879: 100%|██████████| 1/1 [06:18<00:00, 378.19s/it]\n",
            "Epoch:  34| Train loss: 10.68207| NDCG@10: 0.04525| HIT@10: 0.10019: 100%|██████████| 1/1 [06:18<00:00, 378.35s/it]\n",
            "Epoch:  35| Train loss: 10.68128| NDCG@10: 0.04454| HIT@10: 0.09833: 100%|██████████| 1/1 [06:16<00:00, 376.09s/it]\n",
            "  0%|          | 0/1 [03:37<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-0612f618cd3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             data_loader = data_loader)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         ndcg, hit = evaluate(\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0muser_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-db627f9a9257>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, user_train, user_valid, max_len, bert4rec_dataset, make_sequence_dataset)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# gradient를 트래킹하지 않음, 메모리 사용량을 줄이고 연산속도 향상\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-cbc6ba300632>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_seqs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmask_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_seqs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# mask for zero pad / (batch_size, 1, max_len, max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, max_len, num_item + 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-cbc6ba300632>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_enc, mask)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0moutput_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0moutput_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointwise_feedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-cbc6ba300632>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 191\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_list = []\n",
        "ndcg_list = []\n",
        "hit_list = []\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            data_loader = data_loader)\n",
        "        \n",
        "        ndcg, hit = evaluate(\n",
        "            model = model, \n",
        "            user_train = user_train, \n",
        "            user_valid = user_valid, \n",
        "            max_len = config.max_len,\n",
        "            bert4rec_dataset = bert4rec_dataset, \n",
        "            make_sequence_dataset = make_sequence_dataset,\n",
        "            )\n",
        "\n",
        "        loss_list.append(train_loss)\n",
        "        ndcg_list.append(ndcg)\n",
        "        hit_list.append(hit)\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IYFTapfXcvb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
        "ax = ax.flatten()\n",
        "epochs = [i for i in range(1, config.num_epochs + 1)]\n",
        "\n",
        "ax[0].plot(epochs, loss_list)\n",
        "ax[0].set_title('Loss')\n",
        "\n",
        "ax[1].plot(epochs, ndcg_list)\n",
        "ax[1].set_title('NDCG')\n",
        "\n",
        "ax[2].plot(epochs, hit_list)\n",
        "ax[2].set_title('HIT')\n",
        "plt.show()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRRUv2GcwM_I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPY4f823mmZNZa+Ey5WKkMR",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}