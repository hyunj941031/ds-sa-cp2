{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunj941031/ds-sa-cp2/blob/main/models/BERT4Rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vg0sx3_eDd5",
        "outputId": "4a08689e-b497-41a2-abaa-ebf265ccf210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IatRo2Phd6_n",
        "outputId": "cbe0b44b-e4fe-4c7a-ad0b-1999f389649e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-box\n",
            "  Downloading python_box-7.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-7.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaXMbpoEeB03"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "torch.set_printoptions(sci_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls46hifUgmt8"
      },
      "source": [
        "# 1. 학습 parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQr30WC2eQ8E"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : '/content/drive/MyDrive/fashion_campus_dataset',\n",
        "    'max_len' : 100,\n",
        "    'hidden_units' : 500, # Embedding size\n",
        "    'num_heads' : 4, # Multi-head layer 수 (병렬처리)\n",
        "    'num_layers' : 3, # block 수 (encoder layer 수)\n",
        "    'dropout_rate' : 0.4, # dropout의 비율\n",
        "    'lr' : 0.001,\n",
        "    'batch_size' : 128,\n",
        "    'num_epochs' : 50,\n",
        "    'num_workers' : 2,\n",
        "    'mask_prob' : 0.15, # cloze task\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsuqMA2_gqXH"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwHqzLnkgf4I"
      },
      "outputs": [],
      "source": [
        "class MakeSequenceDataSet():\n",
        "    \"\"\"\n",
        "    SequenceData 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'user_item.csv'))\n",
        "\n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('itemId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['itemId'].apply(lambda x : self.item_encoder[x] + 1)\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "        self.df = self.df.sort_values(['user_idx', 'timestamp']) # 시간에 따른 정렬\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col:str) -> dict:\n",
        "        '''\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Return:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        '''\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "        \n",
        "        return encoder, decoder\n",
        "\n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        '''\n",
        "        sequence data 생성\n",
        "\n",
        "        Return:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        '''\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        group_df = self.df.groupby('user_idx')\n",
        "        for user, item in group_df:\n",
        "            users[user].extend(item['item_idx'].tolist())\n",
        "\n",
        "        for user in users:\n",
        "            user_train[user] = users[user][:-1]\n",
        "            user_valid[user] = [users[user][-1]] # 마지막 아이템 예측\n",
        "\n",
        "        return user_train, user_valid\n",
        "\n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZf_2Zq9k4ix"
      },
      "outputs": [],
      "source": [
        "class BERTRecDataSet(Dataset):\n",
        "    def __init__(self, user_train, max_len, num_user, num_item, mask_prob):\n",
        "        self.user_train = user_train\n",
        "        self.max_len = max_len\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.mask_prob = mask_prob\n",
        "        self._all_items = set([i for i in range(1, self.num_item + 1)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_user\n",
        "\n",
        "    def __getitem__(self, user):\n",
        "        user_seq = self.user_train[user]\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for s in user_seq[-self.max_len:]:\n",
        "            prob = np.random.random()\n",
        "            if prob < self.mask_prob:\n",
        "                prob /= self.mask_prob\n",
        "                if prob < 0.8:\n",
        "                    # masking\n",
        "                    tokens.append(self.num_item + 1) # mask_index: num_item + 1, 0: pad, 1~num_item: item index\n",
        "                elif prob < 0.9:\n",
        "                    # noise를 위한 neg item sampling\n",
        "                    tokens.extend(self.random_neg_sampling(rated_item = user_seq, num_item_sample = 1)) # item random sampling\n",
        "                else:\n",
        "                    tokens.append(s)\n",
        "                labels.append(s) # 학습에 사용\n",
        "            else:\n",
        "                tokens.append(s)\n",
        "                labels.append(0) # 학습에 사용 x\n",
        "                \n",
        "        mask_len = self.max_len - len(tokens)\n",
        "        tokens = [0] * mask_len + tokens\n",
        "        labels = [0] * mask_len + labels\n",
        "\n",
        "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
        "\n",
        "    def random_neg_sampling(self, rated_item, num_item_sample : int):\n",
        "        nge_samples = random.sample(list(self._all_items - set(rated_item)), num_item_sample)\n",
        "        return nge_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN1cYr2MoqOk"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaBmHAqnoPbb"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        '''\n",
        "        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n",
        "        mask : (batch_size, 1, max_len, max_len)\n",
        "        '''\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n",
        "        attn_score = attn_score. masked_fill(mask == 0, -1e9) # 유사도가 0인 지점은 -infinity로 보내서 softmax 결과가 0이 아니게 함\n",
        "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1)) # attention distribution\n",
        "        output = torch.matmul(attn_dist, V)\n",
        "        return output, attn_dist\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "        # query, key value, output 생성을 위한 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units * num_heads, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6)\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "        '''\n",
        "        enc : (batch_size, max_len, hidden_units)\n",
        "        mask : (batch_size, 1, max_len, max_len)\n",
        "        '''\n",
        "        residual = enc # residual connection을 위한 residual 저장\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n",
        "\n",
        "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n",
        "        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n",
        "\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
        "        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n",
        "        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n",
        "\n",
        "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n",
        "        return output, attn_dist\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "\n",
        "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
        "        output = self.layerNorm(self.dropout(output) + residual)\n",
        "        return output\n",
        "\n",
        "class BERT4RecBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(BERT4RecBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
        "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
        "\n",
        "    def forward(self, input_enc, mask):\n",
        "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
        "        output_enc = self.pointwise_feedforward(output_enc)\n",
        "        return output_enc, attn_dist\n",
        "\n",
        "class BERT4Rec(nn.Module):\n",
        "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device):\n",
        "        super(BERT4Rec, self).__init__()\n",
        "\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers \n",
        "        self.device = device\n",
        "        \n",
        "        self.item_emb = nn.Embedding(num_item + 2, hidden_units, padding_idx=0) # padding : 0 / item : 1 ~ num_item + 1 /  mask : num_item + 2\n",
        "        self.pos_emb = nn.Embedding(max_len, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([BERT4RecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
        "        self.out = nn.Linear(hidden_units, num_item + 1)\n",
        "    \n",
        "    def forward(self, log_seqs):\n",
        "        \"\"\"\n",
        "        log_seqs : (batch_size, max_len)\n",
        "\n",
        "        ex)\n",
        "        log_seqs = [\n",
        "                [1, 2, 3, 4, 5],\n",
        "                [0, 0, 0, 1, 2],\n",
        "                [0, 0, 1, 2, 3]\n",
        "        ]\n",
        "        \n",
        "        \"\"\"\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device)) # (batch_size, max_len, hidden_units)\n",
        "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1]) # (batch_size, max_len)\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device)) # (batch_size, max_len, hidden_units)\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs)) # LayerNorm\n",
        "\n",
        "        mask_pad = torch.BoolTensor(log_seqs > 0).unsqueeze(1).repeat(1, log_seqs.shape[1], 1).unsqueeze(1).to(self.device) # mask for zero pad / (batch_size, 1, max_len, max_len)\n",
        "        for block in self.blocks:\n",
        "            seqs, attn_dist = block(seqs, mask_pad)\n",
        "        out = self.out(seqs) # (batch_size, max_len, num_item + 1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER_MsqHwET7d"
      },
      "source": [
        "# 4. 학습함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXrvDfLKqkWp"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, data_loader):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for seq, labels in data_loader:\n",
        "        logits = model(seq)\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        labels = labels.view(-1).to(device) # 같은 device에 들어가게끔\n",
        "\n",
        "        optimizer.zero_grad() # 역전파 시, 루프마다 간섭을 하지 않게 하기 위해서 grad값들을 0으로 초기화\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def evaluate(model, user_train, user_valid, max_len, bert4rec_dataset, make_sequence_dataset):\n",
        "    model.eval() # layer들의 동작을 inference(evaluation) mode로 바꿔주는 목적 -> BatchNorm 등\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10 / normalized discounted cumulative gain : 랭킹 결과의 길이에 상관없이 일정한 스케일을 갖도록 하는 normalize\n",
        "    HIT = 0.0 # HIT@10 / 전체 사용자 수 대비 적중한 사용자 수\n",
        "\n",
        "    num_item_sample = 100\n",
        "\n",
        "    users = [user for user in range(make_sequence_dataset.num_user)]\n",
        "\n",
        "    for user in users:\n",
        "        seq = (user_train[user] + [make_sequence_dataset.num_item + 1])[-max_len:]\n",
        "        rated = user_train[user] + user_valid[user]\n",
        "        items = user_valid[user] + bert4rec_dataset.random_neg_sampling(rated_item = rated, num_item_sample = num_item_sample)\n",
        "\n",
        "        with torch.no_grad(): # gradient를 트래킹하지 않음, 메모리 사용량을 줄이고 연산속도 향상\n",
        "            predictions = -model(np.array([seq]))\n",
        "            predictions = predictions[0][-1][items]\n",
        "            rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        if rank < 10: # Top10\n",
        "            NDCG += 1 / np.log2(rank+2)\n",
        "            HIT += 1\n",
        "\n",
        "    NDCG /= len(users)\n",
        "    HIT /= len(users)\n",
        "\n",
        "    return NDCG, HIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxqJK--xKgyb"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63fbu_6GKbbV"
      },
      "outputs": [],
      "source": [
        "make_sequence_dataset = MakeSequenceDataSet(config=config)\n",
        "user_train, user_valid = make_sequence_dataset.get_train_valid_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpe4KS4qWybY"
      },
      "outputs": [],
      "source": [
        "bert4rec_dataset = BERTRecDataSet(\n",
        "    user_train = user_train, \n",
        "    max_len = config.max_len, \n",
        "    num_user = make_sequence_dataset.num_user, \n",
        "    num_item = make_sequence_dataset.num_item,\n",
        "    mask_prob = config.mask_prob,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIIjMMm7XR_v"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(\n",
        "    bert4rec_dataset, \n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = config.num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37s2MFEqXZil"
      },
      "outputs": [],
      "source": [
        "model = BERT4Rec(\n",
        "    num_user = make_sequence_dataset.num_user, \n",
        "    num_item = make_sequence_dataset.num_item, \n",
        "    hidden_units = config.hidden_units, \n",
        "    num_heads = config.num_heads, \n",
        "    num_layers = config.num_layers, \n",
        "    max_len = config.max_len, \n",
        "    dropout_rate = config.dropout_rate, \n",
        "    device = device,\n",
        "    ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjSWoKLmXa45",
        "outputId": "19506a6a-2516-4d97-c792-af0d969ffc3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   1| Train loss: 10.95711| NDCG@10: 0.04504| HIT@10: 0.09940: 100%|██████████| 1/1 [09:42<00:00, 582.44s/it]\n",
            "Epoch:   2| Train loss: 10.78536| NDCG@10: 0.04556| HIT@10: 0.09991: 100%|██████████| 1/1 [09:46<00:00, 586.03s/it]\n",
            "Epoch:   3| Train loss: 10.71164| NDCG@10: 0.04563| HIT@10: 0.09967: 100%|██████████| 1/1 [09:46<00:00, 586.40s/it]\n",
            "Epoch:   4| Train loss: 10.70660| NDCG@10: 0.04440| HIT@10: 0.09776: 100%|██████████| 1/1 [09:43<00:00, 583.91s/it]\n",
            "Epoch:   5| Train loss: 10.70324| NDCG@10: 0.04412| HIT@10: 0.09756: 100%|██████████| 1/1 [10:03<00:00, 603.80s/it]\n",
            "Epoch:   6| Train loss: 10.70130| NDCG@10: 0.04409| HIT@10: 0.09721: 100%|██████████| 1/1 [09:56<00:00, 596.67s/it]\n",
            "Epoch:   7| Train loss: 10.69996| NDCG@10: 0.04465| HIT@10: 0.09869: 100%|██████████| 1/1 [09:48<00:00, 588.06s/it]\n",
            "Epoch:   8| Train loss: 10.69869| NDCG@10: 0.04521| HIT@10: 0.09804: 100%|██████████| 1/1 [09:53<00:00, 593.64s/it]\n",
            "Epoch:   9| Train loss: 10.69625| NDCG@10: 0.04442| HIT@10: 0.09705: 100%|██████████| 1/1 [09:49<00:00, 589.52s/it]\n",
            "Epoch:  10| Train loss: 10.69641| NDCG@10: 0.04435| HIT@10: 0.09778: 100%|██████████| 1/1 [09:53<00:00, 593.38s/it]\n",
            "  0%|          | 0/1 [01:02<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0612f618cd3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         train_loss = train(\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-db627f9a9257>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, data_loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_list = []\n",
        "ndcg_list = []\n",
        "hit_list = []\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            data_loader = data_loader)\n",
        "        \n",
        "        ndcg, hit = evaluate(\n",
        "            model = model, \n",
        "            user_train = user_train, \n",
        "            user_valid = user_valid, \n",
        "            max_len = config.max_len,\n",
        "            bert4rec_dataset = bert4rec_dataset, \n",
        "            make_sequence_dataset = make_sequence_dataset,\n",
        "            )\n",
        "\n",
        "        loss_list.append(train_loss)\n",
        "        ndcg_list.append(ndcg)\n",
        "        hit_list.append(hit)\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-IYFTapfXcvb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
        "ax = ax.flatten()\n",
        "epochs = [i for i in range(1, config.num_epochs + 1)]\n",
        "\n",
        "ax[0].plot(epochs, loss_list)\n",
        "ax[0].set_title('Loss')\n",
        "\n",
        "ax[1].plot(epochs, ndcg_list)\n",
        "ax[1].set_title('NDCG')\n",
        "\n",
        "ax[2].plot(epochs, hit_list)\n",
        "ax[2].set_title('HIT')\n",
        "plt.show()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRRUv2GcwM_I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAJsUEjrhkXfBS2GWsdzEt",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}