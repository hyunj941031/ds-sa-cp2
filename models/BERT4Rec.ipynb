{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunj941031/ds-sa-cp2/blob/main/models/BERT4Rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vg0sx3_eDd5",
        "outputId": "aafa8fd3-1567-4ad5-e9fb-aa9cd8ead51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IatRo2Phd6_n",
        "outputId": "dcdc5c89-b663-44d3-8312-6d5cce166af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-box\n",
            "  Downloading python_box-7.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-7.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zaXMbpoEeB03"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "torch.set_printoptions(sci_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls46hifUgmt8"
      },
      "source": [
        "# 1. 학습 parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kQr30WC2eQ8E"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : '/content/drive/MyDrive/fashion_campus_dataset',\n",
        "    \n",
        "    'sequence_len' : 50,\n",
        "    'mask_prob' : 0.3, # cloze task\n",
        "    'random_seed' : 123,\n",
        "\n",
        "    'num_layers' : 2, # block 수 (encoder layer 수)\n",
        "    'hidden_units' : 50, # Embedding size\n",
        "    'num_heads' : 2, # Multi-head layer 수 (병렬처리), hidden_units를 나눴을 때 나누어 떨어지게게\n",
        "    'dropout' : 0.15, # dropout의 비율\n",
        "\n",
        "    'epoch' : 5,\n",
        "    'patience' : 5,\n",
        "    'batch_size' : 256,\n",
        "    'lr' : 0.001,\n",
        "\n",
        "    'num_epochs' : 10,\n",
        "    'num_workers' : 4,\n",
        "    'val_data' : 3,\n",
        "    'delete_data' : False,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsuqMA2_gqXH"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pwHqzLnkgf4I"
      },
      "outputs": [],
      "source": [
        "class MakeSequenceDataSet():\n",
        "    \"\"\"\n",
        "    SequenceData 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'user_item.csv'))\n",
        "        if config['delete_data']:\n",
        "            self.df = self.delete_ones()\n",
        "\n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('itemId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_items, self.num_users = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['itemId'].apply(lambda x : self.item_encoder[x] + 1)\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "        self.df = self.df.sort_values(['user_idx', 'timestamp']) # 시간에 따른 정렬\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col:str) -> dict:\n",
        "        '''\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Return:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        '''\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "        \n",
        "        return encoder, decoder\n",
        "\n",
        "    def delete_ones(self) -> dict:\n",
        "        a = self.df.groupby('userId')['itemId'].size()\n",
        "        for i in a.index:\n",
        "            if a[i] <= config['val_data']:\n",
        "                del(a[i])\n",
        "        df_ = self.df.copy()\n",
        "        df_ = df_[df_['userId'].isin(a.index)]\n",
        "                \n",
        "        return df_\n",
        "\n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        '''\n",
        "        sequence data 생성\n",
        "\n",
        "        Return:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        '''\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        group_df = self.df.groupby('user_idx')\n",
        "        for user, item in group_df:\n",
        "            users[user].extend(item['item_idx'].tolist())\n",
        "\n",
        "        for user in users:\n",
        "            user_train[user] = users[user][:-config['val_data']]\n",
        "            user_valid[user] = users[user][-config['val_data']:] # 마지막 아이템 예측\n",
        "\n",
        "        return user_train, user_valid\n",
        "\n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iZf_2Zq9k4ix"
      },
      "outputs": [],
      "source": [
        "class BERTTrainDataSet(Dataset):\n",
        "    def __init__(self, user_train, num_users, num_items, sequence_len=200, mask_prob=0.15, random_seed=None):\n",
        "        self.user_train = user_train\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.sequence_len = sequence_len\n",
        "        self.mask_prob = mask_prob\n",
        "        self._all_items = set([i for i in range(1, self.num_items + 1)])\n",
        "\n",
        "        # rng\n",
        "        if random_seed is None:\n",
        "            self.rng = random.Random()\n",
        "        else:\n",
        "            self.rng = random.Random(random_seed)\n",
        "\n",
        "        # tokens\n",
        "        self.mask_token = self.num_items + 1\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # 총 user 수(학습에 사용할 sequence 수수)\n",
        "        return self.num_users\n",
        "\n",
        "    def __getitem__(self, user):\n",
        "        seq = self.user_train[user]\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for s in seq[-self.sequence_len:]:\n",
        "            prob = np.random.random()\n",
        "            if prob < self.mask_prob:\n",
        "                prob /= self.mask_prob\n",
        "                # BERT 학습\n",
        "                # random 하게 80% 를 mask token 으로 변환\n",
        "                if prob < 0.8:\n",
        "                    # masking\n",
        "                    # mask_index : num_item + 1 , 0 : pad, 1 ~ num_item : item index\n",
        "                    tokens.append(self.mask_token)\n",
        "                elif prob < 0.9:\n",
        "                    # item random sampling (noise)\n",
        "                    tokens.append(np.random.randint(1, self.num_items))\n",
        "                else:\n",
        "                    # 나머지 10% 를 original token 으로 사용\n",
        "                    tokens.append(s)\n",
        "                labels.append(s) # 학습에 사용\n",
        "            # training 에 사용하지 않음 \n",
        "            else:\n",
        "                tokens.append(s)\n",
        "                labels.append(0) # 학습에 사용하지 않음, trivial\n",
        "\n",
        "        # zero padding \n",
        "        # tokens 와 labels 가 padding_len 보다 짧으면 zero padding 을 해준다. \n",
        "        padding_len = self.sequence_len - len(tokens)\n",
        "        \n",
        "        tokens = [0] * padding_len + tokens\n",
        "        labels = [0] * padding_len + labels\n",
        "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
        "\n",
        "    def _getseq(self, user):\n",
        "        return self.user_train[user]\n",
        "\n",
        "    def random_neg_sampling(self, sold_items, num_item_sample):\n",
        "        nge_samples = random.sample(list(self._all_items - set(sold_items)), num_item_sample)\n",
        "        return nge_samples\n",
        "\n",
        "class BertEvalDataset(Dataset):\n",
        "    def __init__(self, u2seq, u2answer, sequence_len, mask_token, negative_samples):\n",
        "        self.u2seq = u2seq\n",
        "        self.users = sorted(self.u2seq.keys())\n",
        "        self.u2answer = u2answer\n",
        "        self.sequence_len = sequence_len\n",
        "        self.mask_token = mask_token\n",
        "        self.negative_samples = negative_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user = self.users[index]\n",
        "        seq = self.u2seq[user]\n",
        "        answer = self.u2answer[user]\n",
        "        negs = self.random_neg_sampling(sold_item=user, num_item_sample = 1)\n",
        "\n",
        "        candidates = answer + negs\n",
        "        labels = [1] * len(answer) + [0] * len(negs)\n",
        "\n",
        "        seq = seq + [self.mask_token]\n",
        "        seq = seq[-self.sequence_len:]\n",
        "        padding_len = self.sequence_len - len(seq)\n",
        "        seq = [0] * padding_len + seq\n",
        "\n",
        "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n",
        "    \n",
        "    def random_neg_sampling(self, sold_items, num_item_sample : int):\n",
        "        nge_samples = random.sample(list(self._all_items - set(sold_items)), num_item_sample)\n",
        "        return nge_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN1cYr2MoqOk"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Util"
      ],
      "metadata": {
        "id": "WmG9hPNv0ehl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    cudnn.deterministic = True\n",
        "    cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "MeQuNUuwQOJn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "ihaTaFPBR4Dh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "PV_0pdBnf5wx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    # layer가 많아지면 학습이 잘 안되는 현상\n",
        "    # 따라서, sub-layer들에 각각 dropout을 한 후 서로 residual connection을 적용하고, layer normalization 적용\n",
        "    def __init__(self, hidden_units, dropout):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.norm = LayerNorm(hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, sublayer, x):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        r = self.norm(x)\n",
        "        r = sublayer(r)\n",
        "\n",
        "        r = self.dropout(r)\n",
        "\n",
        "        return x + r"
      ],
      "metadata": {
        "id": "yViBKzGuiXQZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_1 = nn.Linear(hidden_units, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
      ],
      "metadata": {
        "id": "u7zJ2-uOSdnR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "3fXOiUlHq9ck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KaBmHAqnoPbb"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None, dropout=None):\n",
        "        \"\"\"\n",
        "            b = batch, ? = num_heads, L = sequence_len\n",
        "\n",
        "            Q: (b x ? x L x dim_Q)\n",
        "            K: (b x ? x L x dim_K)\n",
        "            V: (b x ? x L x dim_V)\n",
        "            ?: 1 (squeezed) or h (multi-head)\n",
        "            mask: (b x ? x L x L)\n",
        "            dropout: nn.Module\n",
        "            assuming dim_Q = dim_K\n",
        "        \"\"\"\n",
        "\n",
        "        # A: (b x ? x L x L)\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
        "\n",
        "        # apply mask (the logit value of a padding token should be minus infinity)\n",
        "        if mask is not None:\n",
        "            attn_score = attn_score.masked_fill(mask == 0, -1e9) # 유사도가 0인 지점은 -infinity로 보내서 softmax 결과가 0이 아니게 함\n",
        "        \n",
        "        # getting normalized(probability) weights through softmax (when padding token, it'll be 0)\n",
        "        # P: (b x ? x L x L)\n",
        "        p_attn = F.softmax(attn_score, dim=-1)\n",
        "        \n",
        "        # apply dropout (with given dropout)\n",
        "        if dropout is not None:\n",
        "            p_attn = self.dropout(p_attn) # attention distribution 상대적 중요도\n",
        "\n",
        "        # (b x ? x L x L) @ (b x ? x L x dim_V) -> (b x ? x L x dim_V)\n",
        "        output = torch.matmul(p_attn, V)\n",
        "\n",
        "        return output, p_attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout=0.1):\n",
        "\n",
        "        \"\"\"\n",
        "            dim_V should be equal to hidden_units / num_heads\n",
        "            we assume dim_Q = dim_K = dim_V\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        assert hidden_units % num_heads == 0\n",
        "\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.dim_V = hidden_units // num_heads\n",
        "\n",
        "        # query, key, value, output 생성을 위한 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "\n",
        "        self.W_O = nn.Linear(hidden_units * num_heads, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layerNorm = LayerNorm(hidden_units)\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "\n",
        "        residual = enc\n",
        "\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # 1) Do all the linear projections in a batch from dim_model, then split into (num_heads x dim_V)\n",
        "        # [process]\n",
        "        # (1) linear(W): (b x L x dim_model) -> (b x L x dim_model)\n",
        "        # (2) view: (b x L x dim_model) -> (b x L x num_heads x dim_V)\n",
        "        # (3) transpose: (b x L x num_heads x dim_V) -> (b x num_heads x L x dim_V)\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "\n",
        "        # 2) Apply attention to the projected vectors in the batch\n",
        "        # note that attenion only cares about the last two dimensions\n",
        "        # output: (b x num_heads x L x dim_V)\n",
        "        # Head별로 각기 다른 attention이 가능하도록 각각 attention에 통과시킴\n",
        "        output, attn_dist = self.attention(Q, K, V, mask=mask, dropout=self.dropout) # attn_dist : (batch_size, num_heads, sequence_len, sequence_len)\n",
        "\n",
        "        # 3) \"concat\" those heads using view\n",
        "        # [process]\n",
        "        # (1) transpose: (b x num_heads x L x dim_V) -> (b x L x num_heads x dim_V)\n",
        "        # (2) contiguous: reorder memory inside GPU (no dimension change)\n",
        "        # (3) view: (b x L x num_heads x dim_V) -> (b x L x dim_model)\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
        "        output = output.transpose(1, 2).contiguous() # (batch_size, sequence_len, num_heads, d_model) / contiguous() : 가변적 메모리 할당\n",
        "        output = output.view(batch_size, seqlen, -1) # (batch_size, sequence_len, d_model * num_heads)\n",
        "\n",
        "        # 4) apply the final linear\n",
        "        # X: (b x L x dim_model)\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "-Xr1eb1UubmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional Encoder = Transformer (self-attention)\n",
        "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_units=256, num_heads=4, d_ff=1024, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param hidden_units: hidden_units size of transformer\n",
        "        :param num_heads: head sizes of multi-head attention\n",
        "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_units\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, hidden_units=hidden_units, dropout=dropout)\n",
        "        self.attention_sublayer = SublayerConnection(hidden_units=hidden_units, dropout=dropout)\n",
        "        self.pwff = PositionwiseFeedForward(hidden_units=hidden_units, d_ff=d_ff, dropout=dropout)\n",
        "        self.pwff_sublayer = SublayerConnection(hidden_units=hidden_units, dropout=dropout)\n",
        "        self.dropoutlayer = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_enc, mask=None):\n",
        "        # we need dynamic mask for the attention forward (sublayer module also has parameters, namely layernorm)\n",
        "        # x: (b x L x dim_model)\n",
        "        # mask: (b x L x L), set False to ignore that point\n",
        "        x = self.attention_sublayer(lambda _x: self.attention(_x, mask=mask), input_enc)\n",
        "        x = self.pwff_sublayer(self.pwff, x)\n",
        "        return self.dropoutlayer(x)\n",
        "\n",
        "class BERT4Rec(nn.Module):\n",
        "    def __init__(self, num_users, num_items, sequence_len, device, num_layers=2, hidden_units=256, num_heads=4, dropout=0.1, random_seed=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # params\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.sequence_len = sequence_len\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.random_seed = random_seed\n",
        "        self.device = device\n",
        "        \n",
        "        # set seed\n",
        "        if random_seed is not None:\n",
        "            fix_random_seed(random_seed)\n",
        "\n",
        "        # 0: padding token\n",
        "        # 1 ~ V: item tokens\n",
        "        # V + 1: mask token\n",
        "\n",
        "        # Embedding layers\n",
        "        self.item_emb = nn.Embedding(num_items + 2, hidden_units, padding_idx=0) # padding : 0 / item : 1 ~ num_item + 1 /  mask : num_item + 2\n",
        "        self.pos_emb = nn.Embedding(sequence_len, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "\n",
        "        # transformer layers\n",
        "        self.transformers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                hidden_units=hidden_units,\n",
        "                num_heads=num_heads,\n",
        "                d_ff=hidden_units*4,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.out = nn.Linear(hidden_units, num_items + 1)\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        L = tokens.shape[1]\n",
        "\n",
        "        # mask for whether padding token or not in attention matrix (True if padding token)\n",
        "        # [process] (b x L) -> (b x 1 x L) -> (b x L x L) -> (b x 1 x L x L)\n",
        "        token_mask = torch.BoolTensor(tokens > 0).unsqueeze(1).repeat(1, L, 1).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # get embedding\n",
        "        # [process] (b x L) -> (b x L x d)\n",
        "        seqs = self.item_emb(torch.LongTensor(tokens).to(self.device))  # (batch_size, sequence_len, hidden_units)\n",
        "        positions = np.tile(np.array(range(tokens.shape[1])), [tokens.shape[0], 1])  # (batch_size, sequence_len)\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))  # (batch_size, sequence_len, hidden_units)\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs))  # LayerNorm\n",
        "\n",
        "        # apply multi-layered transformers\n",
        "        # [process] (b x L x d) -> ... -> (b x L x d)\n",
        "        for transformer in self.transformers:\n",
        "            seqs = transformer(seqs, token_mask)\n",
        "\n",
        "        # classifier\n",
        "        # [process] (b x L x d) -> (b x L x (V + 1))\n",
        "        out = self.out(seqs)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "xtf_I6wZqtsL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER_MsqHwET7d"
      },
      "source": [
        "# 4. 학습함수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Solver:\n",
        "    def calculate_loss(self, batch):\n",
        "        \n",
        "        self.ce_losser = CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        # device\n",
        "        tokens = batch['tokens'].to(self.device)  # b x L\n",
        "        labels = batch['labels'].to(self.device)  # b x L\n",
        "\n",
        "        # forward\n",
        "        logits = self.model(tokens)  # b x L x (V + 1)\n",
        "\n",
        "        # loss\n",
        "        logits = logits.view(-1, logits.size(-1))  # bL x (V + 1)\n",
        "        labels = labels.view(-1)  # bL\n",
        "        loss = self.ce_losser(logits, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def set_model_mode(self, purpose: str) -> None:\n",
        "        if purpose == 'eval':\n",
        "            self.model = self.model.eval()  # type: ignore\n",
        "        elif purpose == 'train':\n",
        "            self.model = self.model.train()\n",
        "\n",
        "    def solve(self) -> None:\n",
        "        C = self.config\n",
        "        name = C['name']\n",
        "        print(f\"solve {name} start\")\n",
        "\n",
        "        # report new session\n",
        "        self.logger.info(\"-- new solve session started --\")\n",
        "\n",
        "        # report model parameters\n",
        "        numels = []\n",
        "        for parameter in self.model.parameters():\n",
        "            if parameter.requires_grad:\n",
        "                numels.append(parameter.numel())\n",
        "        num_params = sum(numels)\n",
        "        self.logger.info(f\"total parameters:\\t{num_params}\")\n",
        "        with open(os.path.join(self.data_dir, 'meta.json'), 'w') as fp:\n",
        "            json.dump({'num_params': num_params}, fp)\n",
        "\n",
        "        # solve loop\n",
        "        self.early_stop = False\n",
        "        self.patience_counter = 0\n",
        "        self.load_model('train')\n",
        "        for epoch in range(self.start_epoch, C['train']['epoch'] + 1):\n",
        "            self.solve_train(epoch)\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "            if self.early_stop:\n",
        "                break\n",
        "\n",
        "        # solve end\n",
        "        self.load_model('test')\n",
        "        self.solve_test()\n",
        "\n",
        "    def solve_train(self, epoch: int) -> None:\n",
        "            self.train_one_epoch(epoch)\n",
        "            self.evaluate_on_valid(epoch)\n",
        "\n",
        "    def train_one_epoch(self, epoch: int) -> None:\n",
        "        # prepare\n",
        "        self.set_model_mode('train')\n",
        "        epoch_loss_sum = 0.0\n",
        "        epoch_loss_count = 0\n",
        "\n",
        "        # loop\n",
        "        pbar = tqdm(self.train_dataloader)\n",
        "        pbar.set_description(f\"[epoch {epoch} train]\")\n",
        "        for batch in pbar:\n",
        "\n",
        "            # get loss\n",
        "            loss = self.calculate_loss(batch)\n",
        "\n",
        "            # backward\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # step end\n",
        "            epoch_loss_sum += float(loss.data)\n",
        "            epoch_loss_count += 1\n",
        "            pbar.set_postfix(loss=f'{epoch_loss_sum / epoch_loss_count:.4f}')\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        # epoch end\n",
        "        epoch_loss = epoch_loss_sum / epoch_loss_count\n",
        "        self.logger.info('\\t'.join([\n",
        "            f\"[epoch {epoch}]\",\n",
        "            f\"train loss: {epoch_loss:.4f}\"\n",
        "        ]))\n",
        "        self.writer.add_scalar('train_loss', epoch_loss, epoch)\n",
        "\n",
        "    def evaluate_on_valid(self, epoch: int) -> None:\n",
        "        self.logger.info(f\"<evaluate on valid (epoch: {epoch})>\")\n",
        "        C = self.config\n",
        "\n",
        "        # prepare\n",
        "        self.set_model_mode('eval')\n",
        "        ks = sorted(C['metric']['ks'])\n",
        "        pivot = C['metric']['pivot']\n",
        "\n",
        "        # init\n",
        "        result_values = []\n",
        "\n",
        "        # loop\n",
        "        pbar = tqdm(self.valid_dataloader)\n",
        "        pbar.set_description(f\"[epoch {epoch} valid]\")\n",
        "        with torch.no_grad():\n",
        "            for batch in pbar:\n",
        "\n",
        "                # get rankers\n",
        "                rankers = self.calculate_rankers(batch)\n",
        "\n",
        "                # evaluate\n",
        "                labels = batch['labels'].to(self.device)\n",
        "                batch_results = calc_batch_rec_metrics_per_k(rankers, labels, ks)  # type: ignore\n",
        "                result_values.extend(batch_results['values'][pivot])\n",
        "\n",
        "                # verbose\n",
        "                pbar.set_postfix(pivot_score=f'{sum(result_values) / len(result_values):.4f}')\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        # report\n",
        "        pivot_score = sum(result_values) / len(result_values)\n",
        "        self.logger.info('\\t'.join([\n",
        "            f\"[epoch {epoch}]\",\n",
        "            f\"valid pivot score: {pivot_score:.4f}\"\n",
        "        ]))\n",
        "        self.writer.add_scalar('valid_pivot_score', pivot_score, epoch)\n",
        "\n",
        "        # save best\n",
        "        if self.best_score is None or self.best_score < pivot_score:\n",
        "            self.best_score = pivot_score\n",
        "            torch.save(self.model.state_dict(), self.model_path)\n",
        "            self.logger.info(f\"updated best model at epoch {epoch} with pivot score {pivot_score}\")\n",
        "            self.patience_counter = 0\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "        if self.patience_counter >= C['train']['patience']:\n",
        "            self.logger.info(f\"no update for {self.patience_counter} epochs, thus early stopping\")\n",
        "            self.early_stop = True\n",
        "\n",
        "        # save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'best_score': self.best_score,\n",
        "            'model': self.model.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "        }, self.check_path)"
      ],
      "metadata": {
        "id": "X1hDGNZsBofe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "def train_one_epoch(epoch, train_dataloader):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss_sum = 0.0\n",
        "    epoch_loss_count = 0\n",
        "\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    pbar.set_description(f\"[epoch {epoch} train\")\n",
        "    \n",
        "    for batch in pbar:\n",
        "        loss = calculate_loss(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss_sum += float(loss.data)\n",
        "        epoch_loss_count += 1\n",
        "        pbar.set_postfix(loss=f'{epoch_loss_sum / epoch_loss_count:.4f}')\n",
        "    \n",
        "    pbar.close()\n",
        "\n",
        "    epoch_loss = epoch_loss_sum / epoch_loss_count\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "def evaluate_on_valid(epoch, valid_dataloader):\n",
        "    model.eval()\n",
        "    \n",
        "    NDCG = 0.0\n",
        "    HIT = 0.0\n",
        "\n",
        "    num_item_sample = 100\n",
        "    users = [user for user in range(make_sequence_dataset.num_user)]\n",
        "    \n",
        "    result_values = []\n",
        "\n",
        "    pbar = tqdm(valid_dataloader)\n",
        "    pbar.set_description(f\"[epoch {epoch} valid\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            rankers = calculate_rankers(batch)\n",
        "\n",
        "            labels = batch['labels'].to(config['device'])\n",
        "            \n",
        "\n",
        "\n",
        "def train(model, criterion, optimizer, data_loader):\n",
        "    \n",
        "    for epoch in range(1, config['num_epochs'] + 1):\n",
        "        train_loss = train_one_epoch(epoch,data_loader)\n",
        "\n",
        "\n",
        "\n",
        "    for seq, labels in data_loader:\n",
        "        logits = model(seq)\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        labels = labels.view(-1).to(config['device'])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def evaluate(model, user_train, user_valid, max_len, bert4rec_dataset, make_sequence_dataset):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10\n",
        "    HIT = 0.0 # HIT@10\n",
        "\n",
        "    num_item_sample = 100\n",
        "\n",
        "    users = [user for user in range(make_sequence_dataset.num_user)]\n",
        "\n",
        "    for user in users:\n",
        "        seq = (user_train[user] + [make_sequence_dataset.num_item + 1])[-max_len:]\n",
        "        rated = user_train[user] + user_valid[user]\n",
        "        items = user_valid[user] + bert4rec_dataset.random_neg_sampling(rated_item = rated, num_item_sample = num_item_sample)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions = -model(np.array([seq]))\n",
        "            predictions = predictions[0][-1][items] # sampling\n",
        "            rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        if rank < 10: #Top10\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HIT += 1\n",
        "\n",
        "    NDCG /= len(users)\n",
        "    HIT /= len(users)\n",
        "\n",
        "    return NDCG, HIT\n",
        "\n",
        "def calculate_loss(batch):\n",
        "        \n",
        "    ce_losser = CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    # device\n",
        "    tokens = batch['tokens'].to(config['device'])  # b x L\n",
        "    labels = batch['labels'].to(config['device'])  # b x L\n",
        "\n",
        "    # forward\n",
        "    logits = model(tokens)  # b x L x (V + 1)\n",
        "\n",
        "    # loss\n",
        "    logits = logits.view(-1, logits.size(-1))  # bL x (V + 1)\n",
        "    labels = labels.view(-1)  # bL\n",
        "    loss = ce_losser(logits, labels)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def calculate_rankers(batch):\n",
        "\n",
        "    # device\n",
        "    tokens = batch['tokens'].to(config['device'])  # b x L\n",
        "    cands = batch['cands'].to(config['device'])  # b x C\n",
        "\n",
        "    # forward\n",
        "    logits = model(tokens)  # b x L x (V + 1)\n",
        "\n",
        "    # gather\n",
        "    logits = logits[:, -1, :]  # b x (V + 1)\n",
        "    scores = logits.gather(1, cands)  # b x C\n",
        "    rankers = scores.argsort(dim=1, descending=True)\n",
        "\n",
        "    return rankers"
      ],
      "metadata": {
        "id": "Jf2zw6CXzM6o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(scores, labels, k):\n",
        "    scores = scores.cpu()\n",
        "    labels = labels.cpu()\n",
        "    rank = (-scores).argsort(dim=1)\n",
        "    cut = rank[:, :k]\n",
        "    hit = labels.gather(1, cut)\n",
        "    return (hit.sum(1).float() / labels.sum(1).float()).mean().item()\n",
        "\n",
        "\n",
        "def ndcg(scores, labels, k):\n",
        "    scores = scores.cpu()\n",
        "    labels = labels.cpu()\n",
        "    rank = (-scores).argsort(dim=1)\n",
        "    cut = rank[:, :k]\n",
        "    hits = labels.gather(1, cut)\n",
        "    position = torch.arange(2, 2+k)\n",
        "    weights = 1 / torch.log2(position.float())\n",
        "    dcg = (hits.float() * weights).sum(1)\n",
        "    idcg = torch.Tensor([weights[:min(n, k)].sum() for n in labels.sum(1)])\n",
        "    ndcg = dcg / idcg\n",
        "    return ndcg.mean()\n",
        "\n",
        "\n",
        "def recalls_and_ndcgs_for_ks(scores, labels, ks):\n",
        "    metrics = {}\n",
        "\n",
        "    scores = scores.cpu()\n",
        "    labels = labels.cpu()\n",
        "    answer_count = labels.sum(1)\n",
        "    answer_count_float = answer_count.float()\n",
        "    labels_float = labels.float()\n",
        "    rank = (-scores).argsort(dim=1)\n",
        "    cut = rank\n",
        "    for k in sorted(ks, reverse=True):\n",
        "       cut = cut[:, :k]\n",
        "       hits = labels_float.gather(1, cut)\n",
        "       metrics['Recall@%d' % k] = (hits.sum(1) / answer_count_float).mean().item()\n",
        "\n",
        "       position = torch.arange(2, 2+k)\n",
        "       weights = 1 / torch.log2(position.float())\n",
        "       dcg = (hits * weights).sum(1)\n",
        "       idcg = torch.Tensor([weights[:min(n, k)].sum() for n in answer_count])\n",
        "       ndcg = (dcg / idcg).mean()\n",
        "       metrics['NDCG@%d' % k] = ndcg\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "W6TvWguEj-PY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxqJK--xKgyb"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "63fbu_6GKbbV"
      },
      "outputs": [],
      "source": [
        "make_sequence_dataset = MakeSequenceDataSet(config=config)\n",
        "user_train, user_valid = make_sequence_dataset.get_train_valid_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rpe4KS4qWybY"
      },
      "outputs": [],
      "source": [
        "bert4rec_dataset = BERTTrainDataSet(\n",
        "    user_train = user_train, \n",
        "    sequence_len = config.sequence_len, \n",
        "    num_users = make_sequence_dataset.num_users, \n",
        "    num_items = make_sequence_dataset.num_items,\n",
        "    mask_prob = config.mask_prob,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gIIjMMm7XR_v"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(\n",
        "    bert4rec_dataset, \n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = config.num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "37s2MFEqXZil"
      },
      "outputs": [],
      "source": [
        "model = BERT4Rec(\n",
        "    num_users = make_sequence_dataset.num_users, \n",
        "    num_items = make_sequence_dataset.num_items, \n",
        "    hidden_units = config.hidden_units, \n",
        "    num_heads = config.num_heads, \n",
        "    num_layers = config.num_layers, \n",
        "    sequence_len = config.sequence_len, \n",
        "    dropout = config.dropout, \n",
        "    device = device,\n",
        "    ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for seq, lab in data_loader:\n",
        "    pred = model(seq)\n",
        "    print(seq, pred[i], lab, sep='\\n')\n",
        "    break"
      ],
      "metadata": {
        "id": "eQljaPfwfnYU",
        "outputId": "c8cac0c2-eee9-41bf-a1de-dea9a924ba21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ..., 44447, 37123,  5506],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ..., 44447, 44447, 11871],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0]])\n",
            "tensor([[-1.6566e+00, 3.3543e+00, 3.6584e+00,  ..., -2.7032e+00, -6.0943e-01, 3.0195e-01],\n",
            "        [1.9332e+00, -1.0586e+00, -1.6316e-01,  ..., 5.3681e+00, 2.0472e+00, 2.6681e+00],\n",
            "        [8.9383e-02, -3.5729e+00, 2.0344e+00,  ..., -2.7300e-01, -2.0158e-01, -2.6318e+00],\n",
            "        ...,\n",
            "        [-8.8385e-02, -7.0150e-01, -2.4269e-01,  ..., 2.2354e+00, -8.5064e-01, -2.4949e+00],\n",
            "        [-4.5297e-02, -2.4430e+00, 7.4260e-01,  ..., -2.1426e+00, 2.7472e+00, 5.9303e-01],\n",
            "        [9.7934e-01, 4.2039e-01, 1.1757e+00,  ..., -2.2125e+00, 2.6678e-01, 4.1111e-01]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ..., 18311, 22657,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ..., 13551, 28901,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred[1]"
      ],
      "metadata": {
        "id": "op-OBVPA6sDi",
        "outputId": "96127d4c-d393-4440-cfb9-05d9edd9705f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.2259e-01, 4.1390e+00, 3.7724e+00,  ..., -2.1231e+00, -1.6079e+00, -7.8699e-01],\n",
              "        [2.3887e+00, 9.0577e-01, -2.1453e+00,  ..., 4.0155e+00, -3.7214e-01, 1.9359e-01],\n",
              "        [-7.2350e-01, -2.5774e+00, 1.1832e+00,  ..., 4.8048e-01, -3.5936e+00, -1.0287e+00],\n",
              "        ...,\n",
              "        [2.1356e+00, 1.6073e+00, -1.5226e+00,  ..., 2.8461e+00, -3.1481e+00, -1.4775e+00],\n",
              "        [2.8800e+00, -5.1918e-01, 9.2901e-01,  ..., 5.4318e-03, 9.7771e-01, 1.8921e+00],\n",
              "        [2.5868e+00, -1.0443e-01, 1.8268e+00,  ..., -1.8493e+00, -9.6882e-01, -1.0748e+00]],\n",
              "       device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred[1][0]"
      ],
      "metadata": {
        "id": "HXd4XtOb7Ksr",
        "outputId": "11de6ffc-1b8d-4dce-f8c6-918c84332389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.2259e-01, 4.1390e+00, 3.7724e+00,  ..., -2.1231e+00, -1.6079e+00, -7.8699e-01],\n",
              "       device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.shape"
      ],
      "metadata": {
        "id": "qIn0xhTOoISY",
        "outputId": "bc828edd-832c-4b22-c84c-159fae5499c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 50, 44447])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred[:, -1, :].shape"
      ],
      "metadata": {
        "id": "5s1Rk5-0oBGA",
        "outputId": "d678628f-b765-43e6-a81b-08271f64d55e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 44447])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq.shape"
      ],
      "metadata": {
        "id": "fJ5tsoXC7PyU",
        "outputId": "d1e8e05b-d83a-4100-fe8e-32bf8ab38b50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq[1]"
      ],
      "metadata": {
        "id": "mhnesG5L7quT",
        "outputId": "14bcb35b-1ce7-4e0d-8205-19d5a354fd1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0, 23431, 44447, 37123,  5506])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab[1]"
      ],
      "metadata": {
        "id": "T0b3TDfx7u3W",
        "outputId": "52d78396-3d25-4e4e-c10e-2cee2c7dcd07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0, 18311, 22657,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred[1][-1]"
      ],
      "metadata": {
        "id": "KJDke72I_BUH",
        "outputId": "bc7a9254-7851-4e8d-e9a6-2c402565c65e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.5868e+00, -1.0443e-01, 1.8268e+00,  ..., -1.8493e+00, -9.6882e-01, -1.0748e+00],\n",
              "       device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(pred[1][-1]).item()"
      ],
      "metadata": {
        "id": "_6zPEjKA_Oo8",
        "outputId": "2de50bf8-2c2b-4fb7-8081-2a820a17b596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.748798847198486"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred[1][-1].gather(dim=0, max(pred[1][-1]).item())"
      ],
      "metadata": {
        "id": "1EOh22qO77ZR",
        "outputId": "7bc61006-1f3b-409f-d34f-ce5cb624576d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-102-a622448afb5a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pred[1][-1].gather(dim=0, max(pred[1][-1]).item())\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.shape"
      ],
      "metadata": {
        "id": "VT7Ab-M97zE7",
        "outputId": "ba945f80-22cb-4703-b546-9ea86e3efeef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(seq)"
      ],
      "metadata": {
        "id": "pWvCAaHjiIkw",
        "outputId": "6f67a3ae-fa9f-4820-ac7c-f37053619573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pred)"
      ],
      "metadata": {
        "id": "J-4D7mtj9tAW",
        "outputId": "76114376-b4d3-4c89-9f0e-f0c1dd5ae34a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pred)"
      ],
      "metadata": {
        "id": "neODHU3490mg",
        "outputId": "c39b2ef4-443a-40b1-cfe2-1399668b4a63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(seq[0])"
      ],
      "metadata": {
        "id": "nw35YBoE90kp",
        "outputId": "9fef0ddd-79f0-43ce-dc3b-f02f5e15b014",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lab[0])"
      ],
      "metadata": {
        "id": "hPj8MhED90ip",
        "outputId": "13fdd0c3-9bdd-42f8-f518-854a35314a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_sequence_dataset.num_items"
      ],
      "metadata": {
        "id": "u-2XTywu90Mm",
        "outputId": "01e87ca0-b051-492b-92bf-9a841e8a3170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44446"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pbar = tqdm(data_loader)"
      ],
      "metadata": {
        "id": "tXHNNn6OptUu",
        "outputId": "60694d0b-883e-4df0-e70c-8166e4f40db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/199 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in pbar:\n",
        "    print(b)\n",
        "    print(len(b))\n",
        "    break"
      ],
      "metadata": {
        "id": "H_Y43a39ptSt",
        "outputId": "7e3d6943-a14d-4226-aafd-59e90e718cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,  4737, 24910, 44402],\n",
            "        [44447,  4654, 16091,  ...,  1026, 13148, 18565],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ..., 17596, 14296, 12436],\n",
            "        [    0,     0,     0,  ...,     0,  8476, 29152],\n",
            "        [30207, 15148, 15819,  ..., 10185,  4268, 35581]]), tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [ 5380,  4654,     0,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0, 30264]])]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NT8hFSYRptQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFqrnARXptOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Ucx3DjfptLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NR4LLsbIptJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rW1eU4UuptHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EsXDRbehptBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aaa"
      ],
      "metadata": {
        "id": "a2VRXTDUpsfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pred.view(-1, pred.size(-1))))"
      ],
      "metadata": {
        "id": "iPguICJLgloX",
        "outputId": "9ac348c6-a62b-44ac-d35f-279df31ebde7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.view(-1, pred.size(-1))"
      ],
      "metadata": {
        "id": "oE8y31kJhRw9",
        "outputId": "fb30c215-cc57-4652-e20c-135af5eead94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.1391e-02, -3.7845e-01, 3.1527e+00,  ..., 2.3241e-01, -2.7994e+00, -1.0001e+00],\n",
              "        [2.3050e-01, -1.2817e+00, -3.1578e-01,  ..., 2.2859e+00, -3.2966e+00, 3.1298e-01],\n",
              "        [2.9735e+00, 1.3699e+00, 9.6975e-01,  ..., 2.8214e+00, -4.7746e+00, 7.4125e-01],\n",
              "        ...,\n",
              "        [1.0138e+00, 1.3480e+00, -3.0387e+00,  ..., 2.4027e+00, -1.6721e+00, -7.0588e-01],\n",
              "        [3.6297e-01, -1.7887e+00, 1.8810e+00,  ..., 2.7300e+00, 4.0698e+00, 1.2453e+00],\n",
              "        [1.4592e-01, 5.5703e-01, 2.2276e+00,  ..., -1.5259e+00, 2.9055e+00, -3.4215e-01]],\n",
              "       device='cuda:0', grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(lab.view(-1)))"
      ],
      "metadata": {
        "id": "NFZymejshzjw",
        "outputId": "267fc695-86af-4244-e599-ce66cd6b1feb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.view(-1)"
      ],
      "metadata": {
        "id": "sh03Sdngh7Vo",
        "outputId": "9acc0fc9-b34f-474a-a562-723382cb7a96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0, 39308, 14434,  ...,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sold = user_train[900] + user_valid[900]"
      ],
      "metadata": {
        "id": "YSFkjgSu19ZF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item = user_valid[900] + bert4rec_dataset.random_neg_sampling(sold_items = sold, num_item_sample = 100)"
      ],
      "metadata": {
        "id": "L_OnwEfSrYBG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(np.array(seq))"
      ],
      "metadata": {
        "id": "X-5QnYUw2IYp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pred[0][-1])"
      ],
      "metadata": {
        "id": "5Rtqn15i5nX3",
        "outputId": "b8d21122-c5ed-4e85-940a-24ca9fb4524c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44447"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = pred[0][-1][item]"
      ],
      "metadata": {
        "id": "uvu8Pb1S2Tqi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pred[50])"
      ],
      "metadata": {
        "id": "67EBBnJT8h1p",
        "outputId": "7db2dfb8-fddc-485f-a603-0928c4d1f422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b55bf8c7d3fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len() of a 0-d tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mTypeError\u001b[0m: len() of a 0-d tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lab)"
      ],
      "metadata": {
        "id": "IxtnM7Uc8o-6",
        "outputId": "5edbfe39-b5ef-49f5-969f-67a6f0e73a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.argsort().argsort()[0].item()"
      ],
      "metadata": {
        "id": "sBbncXFc5_rd",
        "outputId": "d81c8fdc-40db-4c00-b8e3-bb2acf067fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    ndcg_score(lab.cpu().numpy(), pred.cpu().numpy())"
      ],
      "metadata": {
        "id": "9_YC2cGH7MHn",
        "outputId": "54cbba6e-835e-4a53-d2d3-ededbf1c0b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-c7e68d51d87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mndcg_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mndcg_score\u001b[0;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[1;32m   1651\u001b[0m     \"\"\"\n\u001b[1;32m   1652\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m             )\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    916\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, data_loader):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for seq, labels in data_loader:\n",
        "        logits = model(seq)  # B x T x V\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))  # (B*T) x V\n",
        "        labels = labels.view(-1).to(device)  # B*T\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def evaluate(model, user_train, user_valid, sequence_len, bert4rec_dataset, make_sequence_dataset):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10\n",
        "    HIT = 0.0 # HIT@10\n",
        "\n",
        "    num_item_sample = 100\n",
        "\n",
        "    users = [user for user in range(make_sequence_dataset.num_users)]\n",
        "\n",
        "    for user in users:\n",
        "        seq = (user_train[user] + [make_sequence_dataset.num_items + 1] * config['val_data'])[-sequence_len:]\n",
        "        sold = user_train[user] + user_valid[user]\n",
        "        items = user_valid[user] + bert4rec_dataset.random_neg_sampling(sold_items = sold, num_item_sample = num_item_sample)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions = model(np.array([seq]))\n",
        "            predictions = predictions[0][-1][items] # sampling\n",
        "            rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        for i, r in enumerate(predictions):\n",
        "        if rank < 10: #Top10\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HIT += 1\n",
        "\n",
        "    NDCG /= len(users)\n",
        "    HIT /= len(users)\n",
        "\n",
        "    return NDCG, HIT"
      ],
      "metadata": {
        "id": "ETNnkwbl03H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjSWoKLmXa45",
        "outputId": "be7b823d-003e-4bdb-853b-ab4890466340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 10.40876| NDCG@10: 0.04494| HIT@10: 0.09829:   9%|▊         | 17/199 [1:09:27<12:17:21, 243.09s/it]"
          ]
        }
      ],
      "source": [
        "loss_list = []\n",
        "ndcg_list = []\n",
        "hit_list = []\n",
        "for epoch in range(1, config['num_epochs'] + 1):\n",
        "    model.train()\n",
        "    tbar = tqdm(data_loader)\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            data_loader = data_loader)\n",
        "        \n",
        "        ndcg, hit = evaluate(\n",
        "            model = model, \n",
        "            user_train = user_train, \n",
        "            user_valid = user_valid, \n",
        "            sequence_len = config.sequence_len,\n",
        "            bert4rec_dataset = bert4rec_dataset, \n",
        "            make_sequence_dataset = make_sequence_dataset,\n",
        "            )\n",
        "\n",
        "        loss_list.append(train_loss)\n",
        "        ndcg_list.append(ndcg)\n",
        "        hit_list.append(hit)\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IYFTapfXcvb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
        "ax = ax.flatten()\n",
        "epochs = [i for i in range(1, config.num_epochs + 1)]\n",
        "\n",
        "ax[0].plot(epochs, loss_list)\n",
        "ax[0].set_title('Loss')\n",
        "\n",
        "ax[1].plot(epochs, ndcg_list)\n",
        "ax[1].set_title('NDCG')\n",
        "\n",
        "ax[2].plot(epochs, hit_list)\n",
        "ax[2].set_title('HIT')\n",
        "plt.show()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRRUv2GcwM_I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcHe1g1b9BE9Zh0DP8LGuu",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}