{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunj941031/ds-sa-cp2/blob/main/models/BERT4Rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vg0sx3_eDd5",
        "outputId": "f1687266-88ed-43d7-e024-00a855055db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IatRo2Phd6_n",
        "outputId": "f36d185c-87aa-4977-a9b0-4cb37ed95e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-box in /usr/local/lib/python3.9/dist-packages (7.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "zaXMbpoEeB03"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "torch.set_printoptions(sci_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls46hifUgmt8"
      },
      "source": [
        "# 1. 학습 parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "kQr30WC2eQ8E"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : '/content/drive/MyDrive/fashion_campus_dataset',\n",
        "    \n",
        "    'sequence_len' : 50,\n",
        "    'mask_prob' : 0.3, # cloze task\n",
        "    'random_seed' : 123,\n",
        "\n",
        "    'num_layers' : 2, # block 수 (encoder layer 수)\n",
        "    'hidden_units' : 50, # Embedding size\n",
        "    'num_heads' : 2, # Multi-head layer 수 (병렬처리), hidden_units를 나눴을 때 나누어 떨어지게게\n",
        "    'dropout' : 0.15, # dropout의 비율\n",
        "\n",
        "    'epoch' : 5,\n",
        "    'patience' : 5,\n",
        "    'batch_size' : 256,\n",
        "    'lr' : 0.001,\n",
        "\n",
        "    'num_epochs' : 10,\n",
        "    'num_workers' : 4,\n",
        "    'val_data' : 3,\n",
        "    'delete_data' : True,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsuqMA2_gqXH"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "pwHqzLnkgf4I"
      },
      "outputs": [],
      "source": [
        "class MakeSequenceDataSet():\n",
        "    \"\"\"\n",
        "    SequenceData 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'user_item.csv'), index_col=0)\n",
        "        if config['delete_data']:\n",
        "            self.df = self.delete_ones()\n",
        "\n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('itemId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_items, self.num_users = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['itemId'].apply(lambda x : self.item_encoder[x] + 1)\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "        self.df = self.df.sort_values(['user_idx', 'timestamp']) # 시간에 따른 정렬\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col:str) -> dict:\n",
        "        '''\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Return:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        '''\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "        \n",
        "        return encoder, decoder\n",
        "\n",
        "    def delete_ones(self) -> dict:\n",
        "        a = self.df.groupby('userId')['itemId'].size()\n",
        "        for i in a.index:\n",
        "            if a[i] <= config['val_data']:\n",
        "                del(a[i])\n",
        "        df_ = self.df.copy()\n",
        "        df_ = df_[df_['userId'].isin(a.index)]\n",
        "                \n",
        "        return df_\n",
        "\n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        '''\n",
        "        sequence data 생성\n",
        "\n",
        "        Return:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        '''\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        group_df = self.df.groupby('user_idx')\n",
        "        for user, item in group_df:\n",
        "            users[user].extend(item['item_idx'].tolist())\n",
        "\n",
        "        for user in users:\n",
        "            user_train[user] = users[user][:-config['val_data']]\n",
        "            user_valid[user] = users[user][-config['val_data']:] # 마지막 아이템 예측\n",
        "\n",
        "        return user_train, user_valid\n",
        "\n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "iZf_2Zq9k4ix"
      },
      "outputs": [],
      "source": [
        "class BERTTrainDataSet(Dataset):\n",
        "    def __init__(self, user_train, num_users, num_items, sequence_len=200, mask_prob=0.15, random_seed=None):\n",
        "        self.user_train = user_train\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.sequence_len = sequence_len\n",
        "        self.mask_prob = mask_prob\n",
        "        self._all_items = set([i for i in range(1, self.num_items + 1)])\n",
        "\n",
        "        # rng\n",
        "        if random_seed is None:\n",
        "            self.rng = random.Random()\n",
        "        else:\n",
        "            self.rng = random.Random(random_seed)\n",
        "\n",
        "        # tokens\n",
        "        self.mask_token = self.num_items + 1\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # 총 user 수(학습에 사용할 sequence 수수)\n",
        "        return self.num_users\n",
        "\n",
        "    def __getitem__(self, user):\n",
        "        seq = self.user_train[user]\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        for s in seq[-self.sequence_len:]:\n",
        "            prob = np.random.random()\n",
        "            if prob < self.mask_prob:\n",
        "                prob /= self.mask_prob\n",
        "                # BERT 학습\n",
        "                # random 하게 80% 를 mask token 으로 변환\n",
        "                if prob < 0.8:\n",
        "                    # masking\n",
        "                    # mask_index : num_item + 1 , 0 : pad, 1 ~ num_item : item index\n",
        "                    tokens.append(self.mask_token)\n",
        "                elif prob < 0.9:\n",
        "                    # item random sampling (noise)\n",
        "                    tokens.append(np.random.randint(1, self.num_items))\n",
        "                else:\n",
        "                    # 나머지 10% 를 original token 으로 사용\n",
        "                    tokens.append(s)\n",
        "                labels.append(s) # 학습에 사용\n",
        "            # training 에 사용하지 않음 \n",
        "            else:\n",
        "                tokens.append(s)\n",
        "                labels.append(0) # 학습에 사용하지 않음, trivial\n",
        "\n",
        "        # zero padding \n",
        "        # tokens 와 labels 가 padding_len 보다 짧으면 zero padding 을 해준다. \n",
        "        padding_len = self.sequence_len - len(tokens)\n",
        "        \n",
        "        tokens = [0] * padding_len + tokens\n",
        "        labels = [0] * padding_len + labels\n",
        "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
        "\n",
        "    def _getseq(self, user):\n",
        "        return self.user_train[user]\n",
        "\n",
        "    def random_neg_sampling(self, sold_items, num_item_sample):\n",
        "        nge_samples = random.sample(list(self._all_items - set(sold_items)), num_item_sample)\n",
        "        return nge_samples\n",
        "\n",
        "class BertEvalDataset(Dataset):\n",
        "    def __init__(self, u2seq, u2answer, sequence_len, mask_token, negative_samples):\n",
        "        self.u2seq = u2seq\n",
        "        self.users = sorted(self.u2seq.keys())\n",
        "        self.u2answer = u2answer\n",
        "        self.sequence_len = sequence_len\n",
        "        self.mask_token = mask_token\n",
        "        self.negative_samples = negative_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user = self.users[index]\n",
        "        seq = self.u2seq[user]\n",
        "        answer = self.u2answer[user]\n",
        "        negs = self.random_neg_sampling(sold_item=user, num_item_sample = 1)\n",
        "\n",
        "        candidates = answer + negs\n",
        "        labels = [1] * len(answer) + [0] * len(negs)\n",
        "\n",
        "        seq = seq + [self.mask_token]\n",
        "        seq = seq[-self.sequence_len:]\n",
        "        padding_len = self.sequence_len - len(seq)\n",
        "        seq = [0] * padding_len + seq\n",
        "\n",
        "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n",
        "    \n",
        "    def random_neg_sampling(self, sold_items, num_item_sample : int):\n",
        "        nge_samples = random.sample(list(self._all_items - set(sold_items)), num_item_sample)\n",
        "        return nge_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN1cYr2MoqOk"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Util"
      ],
      "metadata": {
        "id": "WmG9hPNv0ehl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    cudnn.deterministic = True\n",
        "    cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "MeQuNUuwQOJn"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "ihaTaFPBR4Dh"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "PV_0pdBnf5wx"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    # layer가 많아지면 학습이 잘 안되는 현상\n",
        "    # 따라서, sub-layer들에 각각 dropout을 한 후 서로 residual connection을 적용하고, layer normalization 적용\n",
        "    def __init__(self, hidden_units, dropout):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.norm = LayerNorm(hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, sublayer, x):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        r = self.norm(x)\n",
        "        r = sublayer(r)\n",
        "\n",
        "        r = self.dropout(r)\n",
        "\n",
        "        return x + r"
      ],
      "metadata": {
        "id": "yViBKzGuiXQZ"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_1 = nn.Linear(hidden_units, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
      ],
      "metadata": {
        "id": "u7zJ2-uOSdnR"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "3fXOiUlHq9ck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "KaBmHAqnoPbb"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None, dropout=None):\n",
        "        \"\"\"\n",
        "            b = batch, ? = num_heads, L = sequence_len\n",
        "\n",
        "            Q: (b x ? x L x dim_Q)\n",
        "            K: (b x ? x L x dim_K)\n",
        "            V: (b x ? x L x dim_V)\n",
        "            ?: 1 (squeezed) or h (multi-head)\n",
        "            mask: (b x ? x L x L)\n",
        "            dropout: nn.Module\n",
        "            assuming dim_Q = dim_K\n",
        "        \"\"\"\n",
        "\n",
        "        # A: (b x ? x L x L)\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
        "\n",
        "        # apply mask (the logit value of a padding token should be minus infinity)\n",
        "        if mask is not None:\n",
        "            attn_score = attn_score.masked_fill(mask == 0, -1e9) # 유사도가 0인 지점은 -infinity로 보내서 softmax 결과가 0이 아니게 함\n",
        "        \n",
        "        # getting normalized(probability) weights through softmax (when padding token, it'll be 0)\n",
        "        # P: (b x ? x L x L)\n",
        "        p_attn = F.softmax(attn_score, dim=-1)\n",
        "        \n",
        "        # apply dropout (with given dropout)\n",
        "        if dropout is not None:\n",
        "            p_attn = self.dropout(p_attn) # attention distribution 상대적 중요도\n",
        "\n",
        "        # (b x ? x L x L) @ (b x ? x L x dim_V) -> (b x ? x L x dim_V)\n",
        "        output = torch.matmul(p_attn, V)\n",
        "\n",
        "        return output, p_attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout=0.1):\n",
        "\n",
        "        \"\"\"\n",
        "            dim_V should be equal to hidden_units / num_heads\n",
        "            we assume dim_Q = dim_K = dim_V\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        assert hidden_units % num_heads == 0\n",
        "\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.dim_V = hidden_units // num_heads\n",
        "\n",
        "        # query, key, value, output 생성을 위한 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n",
        "\n",
        "        self.W_O = nn.Linear(hidden_units * num_heads, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layerNorm = LayerNorm(hidden_units)\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "\n",
        "        residual = enc\n",
        "\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # 1) Do all the linear projections in a batch from dim_model, then split into (num_heads x dim_V)\n",
        "        # [process]\n",
        "        # (1) linear(W): (b x L x dim_model) -> (b x L x dim_model)\n",
        "        # (2) view: (b x L x dim_model) -> (b x L x num_heads x dim_V)\n",
        "        # (3) transpose: (b x L x num_heads x dim_V) -> (b x num_heads x L x dim_V)\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units).transpose(1, 2)\n",
        "\n",
        "        # 2) Apply attention to the projected vectors in the batch\n",
        "        # note that attenion only cares about the last two dimensions\n",
        "        # output: (b x num_heads x L x dim_V)\n",
        "        # Head별로 각기 다른 attention이 가능하도록 각각 attention에 통과시킴\n",
        "        output, attn_dist = self.attention(Q, K, V, mask=mask, dropout=self.dropout) # attn_dist : (batch_size, num_heads, sequence_len, sequence_len)\n",
        "\n",
        "        # 3) \"concat\" those heads using view\n",
        "        # [process]\n",
        "        # (1) transpose: (b x num_heads x L x dim_V) -> (b x L x num_heads x dim_V)\n",
        "        # (2) contiguous: reorder memory inside GPU (no dimension change)\n",
        "        # (3) view: (b x L x num_heads x dim_V) -> (b x L x dim_model)\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
        "        output = output.transpose(1, 2).contiguous() # (batch_size, sequence_len, num_heads, d_model) / contiguous() : 가변적 메모리 할당\n",
        "        output = output.view(batch_size, seqlen, -1) # (batch_size, sequence_len, d_model * num_heads)\n",
        "\n",
        "        # 4) apply the final linear\n",
        "        # X: (b x L x dim_model)\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "-Xr1eb1UubmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional Encoder = Transformer (self-attention)\n",
        "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_units=256, num_heads=4, d_ff=1024, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param hidden_units: hidden_units size of transformer\n",
        "        :param num_heads: head sizes of multi-head attention\n",
        "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_units\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, hidden_units=hidden_units, dropout=dropout)\n",
        "        self.attention_sublayer = SublayerConnection(hidden_units=hidden_units, dropout=dropout)\n",
        "        self.pwff = PositionwiseFeedForward(hidden_units=hidden_units, d_ff=d_ff, dropout=dropout)\n",
        "        self.pwff_sublayer = SublayerConnection(hidden_units=hidden_units, dropout=dropout)\n",
        "        self.dropoutlayer = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_enc, mask=None):\n",
        "        # we need dynamic mask for the attention forward (sublayer module also has parameters, namely layernorm)\n",
        "        # x: (b x L x dim_model)\n",
        "        # mask: (b x L x L), set False to ignore that point\n",
        "        x = self.attention_sublayer(lambda _x: self.attention(_x, mask=mask), input_enc)\n",
        "        x = self.pwff_sublayer(self.pwff, x)\n",
        "        return self.dropoutlayer(x)\n",
        "\n",
        "class BERT4Rec(nn.Module):\n",
        "    def __init__(self, num_users, num_items, sequence_len, device, num_layers=2, hidden_units=256, num_heads=4, dropout=0.1, random_seed=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # params\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.sequence_len = sequence_len\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.random_seed = random_seed\n",
        "        self.device = device\n",
        "        \n",
        "        # set seed\n",
        "        if random_seed is not None:\n",
        "            fix_random_seed(random_seed)\n",
        "\n",
        "        # 0: padding token\n",
        "        # 1 ~ V: item tokens\n",
        "        # V + 1: mask token\n",
        "\n",
        "        # Embedding layers\n",
        "        self.item_emb = nn.Embedding(num_items + 2, hidden_units, padding_idx=0) # padding : 0 / item : 1 ~ num_item + 1 /  mask : num_item + 2\n",
        "        self.pos_emb = nn.Embedding(sequence_len, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "\n",
        "        # transformer layers\n",
        "        self.transformers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                hidden_units=hidden_units,\n",
        "                num_heads=num_heads,\n",
        "                d_ff=hidden_units*4,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.out = nn.Linear(hidden_units, num_items + 1)\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        L = tokens.shape[1]\n",
        "\n",
        "        # mask for whether padding token or not in attention matrix (True if padding token)\n",
        "        # [process] (b x L) -> (b x 1 x L) -> (b x L x L) -> (b x 1 x L x L)\n",
        "        token_mask = torch.BoolTensor(tokens > 0).unsqueeze(1).repeat(1, L, 1).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # get embedding\n",
        "        # [process] (b x L) -> (b x L x d)\n",
        "        seqs = self.item_emb(torch.LongTensor(tokens).to(self.device))  # (batch_size, sequence_len, hidden_units)\n",
        "        positions = np.tile(np.array(range(tokens.shape[1])), [tokens.shape[0], 1])  # (batch_size, sequence_len)\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))  # (batch_size, sequence_len, hidden_units)\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs))  # LayerNorm\n",
        "\n",
        "        # apply multi-layered transformers\n",
        "        # [process] (b x L x d) -> ... -> (b x L x d)\n",
        "        for transformer in self.transformers:\n",
        "            seqs = transformer(seqs, token_mask)\n",
        "\n",
        "        # classifier\n",
        "        # [process] (b x L x d) -> (b x L x (V + 1))\n",
        "        out = self.out(seqs)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "xtf_I6wZqtsL"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER_MsqHwET7d"
      },
      "source": [
        "# 4. 학습함수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, data_loader):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for seq, labels in data_loader:\n",
        "        logits = model(seq)  # B x T x V\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))  # (B*T) x V\n",
        "        labels = labels.view(-1).to(device)  # B*T\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def evaluate(model, user_train, user_valid, sequence_len, bert4rec_dataset, make_sequence_dataset):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@30\n",
        "    HIT = 0.0 # HIT@30\n",
        "\n",
        "    idcg = 1/math.log2(2) + 1/math.log2(3) + 1/math.log2(4)\n",
        "\n",
        "    users = [user for user in range(make_sequence_dataset.num_users)]\n",
        "\n",
        "    for user in users:\n",
        "        seq = ([0]*sequence_len + user_train[user] + [make_sequence_dataset.num_items + 1] * config['val_data'])[-sequence_len:]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions = model(np.array([seq]))\n",
        "\n",
        "        ndcg, hit = ndcg_hit(predictions, user_valid[user], 10, idcg)\n",
        "        \n",
        "        NDCG += ndcg\n",
        "        HIT += hit\n",
        "\n",
        "    NDCG /= len(users)\n",
        "    HIT /= len(users)\n",
        "\n",
        "    return NDCG, HIT"
      ],
      "metadata": {
        "id": "ETNnkwbl03H3"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_hit(predict, valid, k, idcg):\n",
        "    predict = predict.cpu()\n",
        "    valid = valid\n",
        "    temp = (-predict[0][-config[\"val_data\"]:]).argsort(dim=1)[:, :k].numpy()\n",
        "    rank = []\n",
        "    for i in range(len(temp)):\n",
        "        rank.extend(temp[i])\n",
        "\n",
        "    dcg = 0\n",
        "    hit = 0\n",
        "    for i, r in enumerate(rank):\n",
        "        if r in valid:\n",
        "            dcg += 1 / math.log2(i + 2)\n",
        "            hit = 1\n",
        "\n",
        "    ndcg = dcg / idcg\n",
        "    return ndcg, hit"
      ],
      "metadata": {
        "id": "EpfARiJV1pxy"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxqJK--xKgyb"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "63fbu_6GKbbV"
      },
      "outputs": [],
      "source": [
        "make_sequence_dataset = MakeSequenceDataSet(config=config)\n",
        "user_train, user_valid = make_sequence_dataset.get_train_valid_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "rpe4KS4qWybY"
      },
      "outputs": [],
      "source": [
        "bert4rec_dataset = BERTTrainDataSet(\n",
        "    user_train = user_train, \n",
        "    sequence_len = config.sequence_len, \n",
        "    num_users = make_sequence_dataset.num_users, \n",
        "    num_items = make_sequence_dataset.num_items,\n",
        "    mask_prob = config.mask_prob,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "gIIjMMm7XR_v"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(\n",
        "    bert4rec_dataset, \n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = config.num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "37s2MFEqXZil"
      },
      "outputs": [],
      "source": [
        "model = BERT4Rec(\n",
        "    num_users = make_sequence_dataset.num_users, \n",
        "    num_items = make_sequence_dataset.num_items, \n",
        "    hidden_units = config.hidden_units, \n",
        "    num_heads = config.num_heads, \n",
        "    num_layers = config.num_layers, \n",
        "    sequence_len = config.sequence_len, \n",
        "    dropout = config.dropout, \n",
        "    device = device,\n",
        "    ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjSWoKLmXa45",
        "outputId": "4a65bb62-9822-478d-f731-840dc499db8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/135 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "loss_list = []\n",
        "ndcg_list = []\n",
        "hit_list = []\n",
        "for epoch in range(1, config['num_epochs'] + 1):\n",
        "    model.train()\n",
        "    tbar = tqdm(data_loader)\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            data_loader = data_loader)\n",
        "        \n",
        "        ndcg, hit = evaluate(\n",
        "            model = model, \n",
        "            user_train = user_train, \n",
        "            user_valid = user_valid, \n",
        "            sequence_len = config.sequence_len,\n",
        "            bert4rec_dataset = bert4rec_dataset, \n",
        "            make_sequence_dataset = make_sequence_dataset,\n",
        "            )\n",
        "\n",
        "        loss_list.append(train_loss)\n",
        "        ndcg_list.append(ndcg)\n",
        "        hit_list.append(hit)\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@30: {ndcg:.5f}| HIT@30: {hit:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IYFTapfXcvb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
        "ax = ax.flatten()\n",
        "epochs = [i for i in range(1, config.num_epochs + 1)]\n",
        "\n",
        "ax[0].plot(epochs, loss_list)\n",
        "ax[0].set_title('Loss')\n",
        "\n",
        "ax[1].plot(epochs, ndcg_list)\n",
        "ax[1].set_title('NDCG')\n",
        "\n",
        "ax[2].plot(epochs, hit_list)\n",
        "ax[2].set_title('HIT')\n",
        "plt.show()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRRUv2GcwM_I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNT6fCm2pfNgnOW9RTt6Ed4",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}